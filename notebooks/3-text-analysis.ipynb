{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Language and Texts\n",
    "\n",
    "[David J. Thomas](mailto:dave.a.base@gmail.com), [thePortus.com](http://thePortus.com)<br />\n",
    "Instructor of Ancient History and Digital Humanities,<br />\n",
    "Department of History,<br />\n",
    "[University of South Florida](https://github.com/usf-portal)\n",
    "\n",
    "---\n",
    "\n",
    "## This workbook will...\n",
    "\n",
    "* Use the `dhelp` module to access the `cltk` and `nltk` modules\n",
    "* Preprocess the text for analysis\n",
    "* POS (Part of Speech) tag each text\n",
    "* Perform word counts\n",
    "* Analyze several other features of the charter texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Import Module Dependencies\n",
    "\n",
    "The cell below loads all other Python packages needed. You **must** run this before any other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "from IPython.display import clear_output\n",
    "import nltk\n",
    "from nltk.text import Text, TextCollection\n",
    "import cltk\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from dhelp import LatinText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Time Only Setup\n",
    "\n",
    "The following cell MUST be run the first time you run this on a new computer. This will automatically use the `cltk` module to download training corpora and other necessary linguistic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NLTK) Attempting to import punkt\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "(NLTK) Attempting to import verbnet\n",
      "[nltk_data] Downloading package verbnet to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package verbnet is already up-to-date!\n",
      "(NLTK) Attempting to import wordnet\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "(NLTK) Attempting to import large_grammars\n",
      "[nltk_data] Downloading package large_grammars to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package large_grammars is already up-to-date!\n",
      "(NLTK) Attempting to import averaged_perceptron_tagger\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "(NLTK) Attempting to import maxent_treebank_pos_tagger\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "(NLTK) Attempting to import maxent_ne_chunker\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "(NLTK) Attempting to import universal_tagset\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "(NLTK) Attempting to import words\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "(NLTK) Attempting to import sample_grammars\n",
      "[nltk_data] Downloading package sample_grammars to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package sample_grammars is already up-to-date!\n",
      "(NLTK) Attempting to import book_grammars\n",
      "[nltk_data] Downloading package book_grammars to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package book_grammars is already up-to-date!\n",
      "(NLTK) Attempting to import perluniprops\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /Users/davidthomas/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "(CLTK) Attempting to import latin_text_perseus\n",
      "(CLTK) Attempting to import latin_treebank_perseus\n",
      "(CLTK) Attempting to import latin_text_latin_library\n",
      "(CLTK) Attempting to import phi5\n",
      "Error importing phi5 skipping...\n",
      "(CLTK) Attempting to import phi7\n",
      "Error importing phi7 skipping...\n",
      "(CLTK) Attempting to import latin_proper_names_cltk\n",
      "(CLTK) Attempting to import latin_models_cltk\n",
      "(CLTK) Attempting to import latin_pos_lemmata_cltk\n",
      "(CLTK) Attempting to import latin_treebank_index_thomisticus\n",
      "(CLTK) Attempting to import latin_lexica_perseus\n",
      "(CLTK) Attempting to import latin_training_set_sentence_cltk\n",
      "(CLTK) Attempting to import latin_word2vec_cltk\n",
      "(CLTK) Attempting to import latin_text_antique_digiliblt\n",
      "(CLTK) Attempting to import latin_text_corpus_grammaticorum_latinorum\n",
      "(CLTK) Attempting to import latin_text_poeti_ditalia\n"
     ]
    }
   ],
   "source": [
    "# install required nltk linguistic packages\n",
    "nltk_packages = [\n",
    "    'punkt', 'verbnet', 'wordnet', 'large_grammars', 'averaged_perceptron_tagger',\n",
    "    'maxent_treebank_pos_tagger', 'maxent_ne_chunker', 'universal_tagset', 'words',\n",
    "    'sample_grammars', 'book_grammars', 'perluniprops'\n",
    "]\n",
    "for nltk_package in nltk_packages:\n",
    "    try:\n",
    "        print('(NLTK) Attempting to import', nltk_package)\n",
    "        nltk.download(nltk_package)\n",
    "    except Exception as e:\n",
    "        print('Error importing', nltk_package, 'skipping...')\n",
    "\n",
    "# install latin linguistic packages\n",
    "for corpus in CorpusImporter('latin').list_corpora:\n",
    "    try:\n",
    "        print('(CLTK) Attempting to import', corpus)\n",
    "        CorpusImporter('latin').import_corpus(corpus)\n",
    "    except Exception as e:\n",
    "        print('Error importing', corpus, 'skipping...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Configure Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models created successfully.\n"
     ]
    }
   ],
   "source": [
    "class Charter(collections.UserString):\n",
    "    id = None\n",
    "    description = None\n",
    "    archive = None\n",
    "    language = None\n",
    "    scholarly_date_avg = None\n",
    "    \n",
    "    def __init__(self, csv_data_row):\n",
    "        # call parent class init function since we are overriding it\n",
    "        super().__init__(str)\n",
    "        self.id = csv_data_row['id']\n",
    "        self.description = csv_data_row['description']\n",
    "        self.archive = csv_data_row['archive']\n",
    "        self.language = csv_data_row['language']\n",
    "        self.scholarly_date_avg = csv_data_row['scholarly_date_avg']\n",
    "        self.data = csv_data_row['text']\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.data\n",
    "    \n",
    "    @property\n",
    "    def text_clean(self):\n",
    "        \"\"\"Basic text pre-processing, removes stopwords, extra spaces, and numbers, adds macrons\"\"\"\n",
    "        stopwords = []\n",
    "        with open('stopwords_latin.txt') as text_file:\n",
    "            for line in text_file.readlines():\n",
    "                stopwords.append(line)\n",
    "        remove_chars = ['.', ',', ';', ':', '+', '-']\n",
    "        altered_text = self.data\n",
    "        for remove_char in remove_chars:\n",
    "            altered_text = altered_text.replace(remove_char, '')\n",
    "        return LatinText(altered_text.lower()\n",
    "            ).rm_lines(\n",
    "            ).normalize(\n",
    "            ).rm_stopwords(stopwords\n",
    "            ).rm_spaces()\n",
    "    \n",
    "    @property\n",
    "    def text_lemmatized(self):\n",
    "        \"\"\"Gets the clean form of the text then transforms all words to their lemmata.\"\"\"\n",
    "        return self.text_clean.lemmatize()\n",
    "    \n",
    "    @property\n",
    "    def entities(self):\n",
    "        \"\"\"Scans text with cltk's entity recognition and returns a list.\"\"\"\n",
    "        return LatinText(self.text).entities()\n",
    "    \n",
    "    def longest_common_substring(self, other_string):\n",
    "        \"\"\"Returns the longest substring that this and another charter share.\"\"\"\n",
    "        return LatinText(self.data).longest_common_substring(other_string)\n",
    "    \n",
    "    def compare_minhash(self, other_string):\n",
    "        \"\"\"Compares the text minhash similarity of this and another charter.\"\"\"\n",
    "        return LatinText(self.data).compare_minhash(other_string)\n",
    "        \n",
    "    def word_count(self):\n",
    "        \"\"\"Gives a dictionary, each key is a word appearing and the value is the count.\"\"\"\n",
    "        return LatinText(str(self.text_lemmatized)).word_count()\n",
    "    \n",
    "    def word_count_raw(self):\n",
    "        \"\"\"Same as .word_count(), but does not lemmatize words before counting.\"\"\"\n",
    "        return LatinText(self.data).word_count()\n",
    "    \n",
    "    def clausulae_count(self):\n",
    "        \"\"\"Similar to word_count, but instead uses cltk to look for poetic clausulae in prose text.\"\"\"\n",
    "        return LatinText(self.data).clausulae()\n",
    "    \n",
    "    \n",
    "class CharterCorpus(collections.UserList):\n",
    "    \"\"\"List of individual charter objects, provides methods to aid analysis of the documents.\"\"\"\n",
    "    \n",
    "    def __init__(self, charters):\n",
    "        # call parent class init function since we are overriding it\n",
    "        super().__init__()\n",
    "        # ensure that a list was passed\n",
    "        try:\n",
    "            iter(charters)\n",
    "        except:\n",
    "            raise Exception('CharterCorpus must be populated an iterable')\n",
    "        # go through each item and manually append it to the internal list\n",
    "        for charter in charters:\n",
    "            self.data.append(charter)\n",
    "            \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"Loads a csv with charter data.\"\"\"\n",
    "        data_rows = []\n",
    "        with open(filepath, mode='r+', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                data_rows.append(Charter(row))\n",
    "        return cls(data_rows)\n",
    "            \n",
    "    @property\n",
    "    def charter_ids(self):\n",
    "        \"\"\"Returns a new list containings ids of all charters in this list of charter objects.\"\"\"\n",
    "        id_list = []\n",
    "        for charter_obj in self:\n",
    "            id_list.append(charter_obj.id)\n",
    "        return id_list\n",
    "    \n",
    "    def get_by_id(self, charter_id):\n",
    "        \"\"\"Returns a single instance of Charter.\"\"\"\n",
    "        # iterate each charter\n",
    "        for charter in self:\n",
    "            # if match, immediately return charter\n",
    "            if charter.id == charter_id:\n",
    "                return charter\n",
    "        # if no match found, return None\n",
    "        return None\n",
    "    \n",
    "    def get_by_ids(self, id_list):\n",
    "        \"\"\"Returns a new instance of CharterCorpus, populated only with charters matching the id_list.\"\"\"\n",
    "        filtered_charters = []\n",
    "        # iterate each charter\n",
    "        for charter_obj in self:\n",
    "            # if if id matches, if so, add it to a new list of objects\n",
    "            if charter_obj.id in id_list:\n",
    "                filtered_charters.append(charter_obj)\n",
    "        # use self.__class__ to construct a new instance, rather than CharterCorpus(filtered_charters)\n",
    "        return self.__class__(filtered_charters)\n",
    "    \n",
    "    def minhash_distances(self, print_updates=False):\n",
    "        \"\"\"Returns dict with ids as keys and vals are dicts with keys/vals of ids/dists to other charters. e.g...\n",
    "        {'id 1': {'id 2': 0.5, 'id 3': 0.2}, 'id 2': {'id 1': 0.5, 'id 3': 0.8}, 'id 3': {'id 1': 0.2, 'id 2': 0.8}}\n",
    "        \"\"\"\n",
    "        distance_dict = {}\n",
    "        counter = 0\n",
    "        # create empty dicts inside for each charter\n",
    "        for charter_id in self.charter_ids:\n",
    "            distance_dict[charter_id] = {}\n",
    "        # start looping through each charter\n",
    "        for charter in self:\n",
    "            # if silent is not flagged, clear cell and print info for new charter\n",
    "            if print_updates:\n",
    "                clear_output()\n",
    "                print('Working on minhash distances for {} ({}/{}) '.format(charter.id, counter + 1, len(self)), end='')\n",
    "            # start sublooping through other charters to compare against\n",
    "            for other_charter in self:\n",
    "                # skip to the next item if the charters are the same\n",
    "                if charter.id == other_charter.id:\n",
    "                    continue\n",
    "                # computer the value with compare_minhash and store it in the dict\n",
    "                distance_dict[charter.id][other_charter.id] = charter.compare_minhash(str(other_charter.data))\n",
    "            counter += 1\n",
    "        # if silent not flagged, print finished message\n",
    "        if print_updates:\n",
    "            print(' Done!')\n",
    "        return distance_dict\n",
    "\n",
    "\n",
    "print('Models created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on minhash distances for S1442 (347/347)  Done!\n",
      "S1 -> S2: 0.3012326656394453\n",
      "S1 -> S3: 0.30495356037151705\n",
      "S1 -> S4: 0.3002336448598131\n",
      "S1 -> S7: 0.3141592920353982\n",
      "S1 -> S8: 0.2926434923201294\n",
      "S1 -> S9: 0.30641330166270786\n",
      "S2 -> S1: 0.3012326656394453\n",
      "S2 -> S3: 0.5206812652068127\n",
      "S2 -> S4: 0.37763833428408444\n",
      "S2 -> S7: 0.42310469314079424\n",
      "S2 -> S8: 0.3224852071005917\n",
      "S2 -> S9: 0.39107413010590014\n",
      "S3 -> S1: 0.30495356037151705\n",
      "S3 -> S2: 0.5206812652068127\n",
      "S3 -> S4: 0.40069686411149824\n",
      "S3 -> S7: 0.4117647058823529\n",
      "S3 -> S8: 0.3029197080291971\n",
      "S3 -> S9: 0.3640416047548291\n",
      "S4 -> S1: 0.3002336448598131\n",
      "S4 -> S2: 0.37763833428408444\n",
      "S4 -> S3: 0.40069686411149824\n",
      "S4 -> S7: 0.4298745724059293\n",
      "S4 -> S8: 0.2909494725152693\n",
      "S4 -> S9: 0.36945244956772333\n",
      "S7 -> S1: 0.3141592920353982\n",
      "S7 -> S2: 0.42310469314079424\n",
      "S7 -> S3: 0.4117647058823529\n",
      "S7 -> S4: 0.4298745724059293\n",
      "S7 -> S8: 0.3699927166788055\n",
      "S7 -> S9: 0.47593582887700536\n",
      "S8 -> S1: 0.2926434923201294\n",
      "S8 -> S2: 0.3224852071005917\n",
      "S8 -> S3: 0.3029197080291971\n",
      "S8 -> S4: 0.2909494725152693\n",
      "S8 -> S7: 0.3699927166788055\n",
      "S8 -> S9: 0.3847980997624703\n"
     ]
    }
   ],
   "source": [
    "charter_corpus = CharterCorpus.load('../export/raw_charters.csv')\n",
    "\n",
    "minhash_distances = charter_corpus.minhash_distances(print_updates=True)\n",
    "\n",
    "charter_key_counter = 0\n",
    "for charter_key in minhash_distances:\n",
    "    if charter_key_counter > 5:\n",
    "        break\n",
    "    sub_charter_key_counter = 0\n",
    "    for sub_charter_key in minhash_distances[charter_key]:\n",
    "        if sub_charter_key_counter > 5:\n",
    "            break\n",
    "        print('{} -> {}: {}'.format(charter_key, sub_charter_key, minhash_distances[charter_key][sub_charter_key]))\n",
    "        sub_charter_key_counter += 1\n",
    "    charter_key_counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_charters = []\n",
    "lemmatized_charters = []\n",
    "\n",
    "# open session, read texts from db and store in charters\n",
    "session = sessionmaker(bind=engine)()\n",
    "\n",
    "print('Preparing texts (lemmatizing and cleaning)...', end='')\n",
    "counter = 0\n",
    "for charter_obj in session.query(Charter):\n",
    "    counter += 1\n",
    "    cleaned_charters.append(Text(charter_obj.text_clean.tokenize()))\n",
    "    lemmatized_charters.append(Text(charter_obj.text_lemmatized.tokenize()))\n",
    "    if counter % 10 == 0:\n",
    "        print('.', end='')\n",
    "session.close()\n",
    "print('Done!')\n",
    "\n",
    "# converts charters from list of texts into text collection\n",
    "cleaned_charters = TextCollection(cleaned_charters)\n",
    "lemmatized_charters = TextCollection(lemmatized_charters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatized_charters.dispersion_plot(['iesu', 'christi', 'rex', 'deus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iesu')\n",
    "print('Frequency to Document Ratio {}%'.format(round(lemmatized_charters.idf('iesu') * 100, 2)))\n",
    "\n",
    "print('Concordance: First 10 appearances')\n",
    "for concordance_appearance in lemmatized_charters.concordance_list('iesu')[0:10]:\n",
    "    print(concordance_appearance.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Common Contexts')\n",
    "print('\\n---\\nIesu:')\n",
    "print(lemmatized_charters.common_contexts(['iesu']))\n",
    "print('\\n---\\nRex:')\n",
    "print(lemmatized_charters.common_contexts(['rex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_charters.plot(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORE COMING SOON\n",
    "\n",
    "For now, try out the network analysis module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
