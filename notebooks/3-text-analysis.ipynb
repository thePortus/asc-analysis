{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Language and Texts\n",
    "\n",
    "[David J. Thomas](mailto:dave.a.base@gmail.com), [thePortus.com](http://thePortus.com)<br />\n",
    "Instructor of Ancient History and Digital Humanities,<br />\n",
    "Department of History,<br />\n",
    "[University of South Florida](https://github.com/usf-portal)\n",
    "\n",
    "---\n",
    "\n",
    "## This workbook will...\n",
    "\n",
    "* Use the `dhelp` module to access the `cltk` and `nltk` modules\n",
    "* Preprocess the text for analysis\n",
    "* POS (Part of Speech) tag each text\n",
    "* Perform word counts\n",
    "* Analyze several other features of the charter texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Import Module Dependencies\n",
    "\n",
    "The cell below loads all other Python packages needed. You **must** run this before any other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import UserList\n",
    "from IPython.display import clear_output\n",
    "from nltk.text import Text, TextCollection\n",
    "from dhelp import LatinText\n",
    "\n",
    "DB_PATH = 'sqlite://'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Time Only Setup\n",
    "\n",
    "The following cell MUST be run the first time you run this on a new computer. This will automatically use the `cltk` module to download training corpora and other necessary linguistic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs nltk stopwords module\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# installs latin corpora/linguistic trainers\n",
    "LatinText('').setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Configure Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sql.create_engine(DB_PATH, encoding='utf-8')\n",
    "Base = declarative_base()\n",
    "    \n",
    "\n",
    "class Charter(Base):\n",
    "    __tablename__ = 'charters'\n",
    "\n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    archive = sql.Column(sql.String)\n",
    "    language = sql.Column(sql.String)\n",
    "    scholarly_date_avg = sql.Column(sql.Float)\n",
    "    text = sql.Column(sql.Text)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # print full description with id and preview of text up to 30th char\n",
    "        try:\n",
    "            return '<class \\'Charter\\' id=\\'{}\\' text=\\'{}...\\'>'.format(self.id, self.text[0:60])\n",
    "        # if text is shorter than 30 chars, print full description with entire text\n",
    "        except:\n",
    "            return '<class \\'Charter\\' id=\\'{}\\' tex\\'{}\\'>'.format(self.id, self.text)\n",
    "    \n",
    "    @property\n",
    "    def text_clean(self):\n",
    "        \"\"\"Basic text pre-processing, removes stopwords, extra spaces, and numbers, adds macrons\"\"\"\n",
    "        stopwords = []\n",
    "        with open('stopwords_latin.txt') as text_file:\n",
    "            for line in text_file.readlines():\n",
    "                stopwords.append(line)\n",
    "        remove_chars = ['.', ',', ';', ':', '+', '-']\n",
    "        altered_text = self.text\n",
    "        for remove_char in remove_chars:\n",
    "            altered_text = altered_text.replace(remove_char, '')\n",
    "        return LatinText(altered_text.lower()\n",
    "            ).rm_lines(\n",
    "            ).normalize(\n",
    "            ).rm_stopwords(stopwords\n",
    "            ).rm_spaces()\n",
    "    \n",
    "    @property\n",
    "    def text_lemmatized(self):\n",
    "        \"\"\"Gets the clean form of the text then transforms all words to their lemmata.\"\"\"\n",
    "        return self.text_clean.lemmatize()\n",
    "    \n",
    "    @property\n",
    "    def entities(self):\n",
    "        \"\"\"Scans text with cltk's entity recognition and returns a list.\"\"\"\n",
    "        return LatinText(self.text).entities()\n",
    "    \n",
    "    def longest_common_substring(self, other_string):\n",
    "        \"\"\"Returns the longest substring that this and another charter share.\"\"\"\n",
    "        return LatinText(self.text).longest_common_substring(other_string)\n",
    "    \n",
    "    def compare_minhash(self, other_string):\n",
    "        \"\"\"Compares the text minhash similarity of this and another charter.\"\"\"\n",
    "        return LatinText(self.text).compare_minhash(other_string)\n",
    "        \n",
    "    def word_count(self):\n",
    "        \"\"\"Gives a dictionary, each key is a word appearing and the value is the count.\"\"\"\n",
    "        return LatinText(str(self.text_lemmatized)).word_count()\n",
    "    \n",
    "    def word_count_raw(self):\n",
    "        \"\"\"Same as .word_count(), but does not lemmatize words before counting.\"\"\"\n",
    "        return LatinText(self.text).word_count()\n",
    "    \n",
    "    def clausulae_count(self):\n",
    "        \"\"\"Similar to word_count, but instead uses cltk to look for poetic clausulae in prose text.\"\"\"\n",
    "        return LatinText(self.text).clausulae()\n",
    "    \n",
    "    \n",
    "class CharterCorpus(UserList):\n",
    "    \"\"\"List of individual charter objects, provides methods to aid analysis of the documents.\"\"\"\n",
    "    \n",
    "    def __init__(self, charter_objs):\n",
    "        # call parent class init function since we are overriding it\n",
    "        super().__init__()\n",
    "        # ensure that a list was passed\n",
    "        try:\n",
    "            iter(charter_objs)\n",
    "        except:\n",
    "            raise Exception('CharterCorpus must be populated an iterable')\n",
    "        # go through each item and manually append it to the internal list\n",
    "        for charter_obj in charter_objs:\n",
    "            self.data.append(charter_obj)\n",
    "            \n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        \"\"\"Queries the db, returns CharterCorpus with all charters. Usage: `CharterCorpus.load()`.\"\"\"\n",
    "        # create placeholder list\n",
    "        charter_list = []\n",
    "        # open session, read texts from db and store in charters\n",
    "        session = sessionmaker(bind=engine)()\n",
    "        for charter_obj in session.query(Charter):\n",
    "            charter_list.append(charter_obj)\n",
    "        # close session to free memory and db file\n",
    "        session.close()\n",
    "        # create new instance of this class with charter objects\n",
    "        return cls(charter_list)\n",
    "            \n",
    "    @property\n",
    "    def charter_ids(self):\n",
    "        \"\"\"Returns a new list containings ids of all charters in this list of charter objects.\"\"\"\n",
    "        id_list = []\n",
    "        for charter_obj in self:\n",
    "            id_list.append(charter_obj.id)\n",
    "        return id_list\n",
    "    \n",
    "    def get_by_id(self, charter_id):\n",
    "        \"\"\"Returns a single instance of Charter.\"\"\"\n",
    "        # iterate each charter\n",
    "        for charter in self:\n",
    "            # if match, immediately return charter\n",
    "            if charter.id == charter_id:\n",
    "                return charter\n",
    "        # if no match found, return None\n",
    "        return None\n",
    "    \n",
    "    def get_by_ids(self, id_list):\n",
    "        \"\"\"Returns a new instance of CharterCorpus, populated only with charters matching the id_list.\"\"\"\n",
    "        filtered_charters = []\n",
    "        # iterate each charter\n",
    "        for charter_obj in self:\n",
    "            # if if id matches, if so, add it to a new list of objects\n",
    "            if charter_obj.id in id_list:\n",
    "                filtered_charters.append(charter_obj)\n",
    "        # use self.__class__ to construct a new instance, rather than CharterCorpus(filtered_charters)\n",
    "        return self.__class__(filtered_charters)\n",
    "    \n",
    "    def minhash_distances(self, print_updates=False):\n",
    "        \"\"\"Returns dict with ids as keys and vals are dicts with keys/vals of ids/dists to other charters. e.g...\n",
    "        {'id 1': {'id 2': 0.5, 'id 3': 0.2}, 'id 2': {'id 1': 0.5, 'id 3': 0.8}, 'id 3': {'id 1': 0.2, 'id 2': 0.8}}\n",
    "        \"\"\"\n",
    "        distance_dict = {}\n",
    "        counter = 0\n",
    "        # create empty dicts inside for each charter\n",
    "        for charter_id in self.charter_ids:\n",
    "            distance_dict[charter_id] = {}\n",
    "        # start looping through each charter\n",
    "        for charter in self:\n",
    "            # if silent is not flagged, clear cell and print info for new charter\n",
    "            if print_updates:\n",
    "                clear_output()\n",
    "                print('Working on minhash distances for {} ({}/{}) '.format(charter.id, counter + 1, len(self)), end='')\n",
    "            # start sublooping through other charters to compare against\n",
    "            for other_charter in self:\n",
    "                # skip to the next item if the charters are the same\n",
    "                if charter.id == other_charter.id:\n",
    "                    continue\n",
    "                # computer the value with compare_minhash and store it in the dict\n",
    "                distance_dict[charter.id][other_charter.id] = charter.compare_minhash(str(other_charter.text))\n",
    "            counter += 1\n",
    "        # if silent not flagged, print finished message\n",
    "        if print_updates:\n",
    "            print(' Done!')\n",
    "        return distance_dict\n",
    "\n",
    "\n",
    "print('Models created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charter_corpus = CharterCorpus.load()\n",
    "\n",
    "minhash_distances = charter_corpus.minhash_distances(print_updates=True)\n",
    "\n",
    "charter_key_counter = 0\n",
    "for charter_key in minhash_distances:\n",
    "    if charter_key_counter > 5:\n",
    "        break\n",
    "    sub_charter_key_counter = 0\n",
    "    for sub_charter_key in minhash_distances[charter_key]:\n",
    "        if sub_charter_key_counter > 5:\n",
    "            break\n",
    "        print('{} -> {}: {}'.format(charter_key, sub_charter_key, minhash_distances[charter_key][sub_charter_key]))\n",
    "        sub_charter_key_counter += 1\n",
    "    charter_key_counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_charters = []\n",
    "lemmatized_charters = []\n",
    "\n",
    "# open session, read texts from db and store in charters\n",
    "session = sessionmaker(bind=engine)()\n",
    "\n",
    "print('Preparing texts (lemmatizing and cleaning)...', end='')\n",
    "counter = 0\n",
    "for charter_obj in session.query(Charter):\n",
    "    counter += 1\n",
    "    cleaned_charters.append(Text(charter_obj.text_clean.tokenize()))\n",
    "    lemmatized_charters.append(Text(charter_obj.text_lemmatized.tokenize()))\n",
    "    if counter % 10 == 0:\n",
    "        print('.', end='')\n",
    "session.close()\n",
    "print('Done!')\n",
    "\n",
    "# converts charters from list of texts into text collection\n",
    "cleaned_charters = TextCollection(cleaned_charters)\n",
    "lemmatized_charters = TextCollection(lemmatized_charters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatized_charters.dispersion_plot(['iesu', 'christi', 'rex', 'deus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iesu')\n",
    "print('Frequency to Document Ratio {}%'.format(round(lemmatized_charters.idf('iesu') * 100, 2)))\n",
    "\n",
    "print('Concordance: First 10 appearances')\n",
    "for concordance_appearance in lemmatized_charters.concordance_list('iesu')[0:10]:\n",
    "    print(concordance_appearance.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Common Contexts')\n",
    "print('\\n---\\nIesu:')\n",
    "print(lemmatized_charters.common_contexts(['iesu']))\n",
    "print('\\n---\\nRex:')\n",
    "print(lemmatized_charters.common_contexts(['rex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_charters.plot(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORE COMING SOON\n",
    "\n",
    "For now, try out the network analysis module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
