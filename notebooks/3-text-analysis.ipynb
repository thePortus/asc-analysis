{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Language and Texts\n",
    "\n",
    "[David J. Thomas](mailto:dave.a.base@gmail.com), [thePortus.com](http://thePortus.com)<br />\n",
    "Instructor of Ancient History and Digital Humanities,<br />\n",
    "Department of History,<br />\n",
    "[University of South Florida](https://github.com/usf-portal)\n",
    "\n",
    "---\n",
    "\n",
    "## This workbook will...\n",
    "\n",
    "* Use the `dhelp` module to access the `cltk` and `nltk` modules\n",
    "* Preprocess the text for analysis\n",
    "* POS (Part of Speech) tag each text\n",
    "* Perform word counts\n",
    "* Analyze several other features of the charter texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Import Module Dependencies\n",
    "\n",
    "The cell below loads all other Python packages needed. You **must** run this before any other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from nltk.text import Text, TextCollection\n",
    "from dhelp import LatinText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Time Only Setup\n",
    "\n",
    "The following cell MUST be run the first time you run this on a new computer. This will automatically use the `cltk` module to download training corpora and other necessary linguistic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs nltk stopwords module\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# installs latin corpora/linguistic trainers\n",
    "LatinText('').setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Configure Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sql.create_engine('sqlite:///charters.db', encoding='utf-8')\n",
    "Base = declarative_base()\n",
    "    \n",
    "\n",
    "class Charter(Base):\n",
    "    __tablename__ = 'charters'\n",
    "\n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    archive = sql.Column(sql.String)\n",
    "    language = sql.Column(sql.String)\n",
    "    scholarly_date_avg = sql.Column(sql.Float)\n",
    "    text = sql.Column(sql.Text)\n",
    "    \n",
    "    @property\n",
    "    def text_clean(self):\n",
    "        \"\"\"Basic text pre-processing, removes stopwords, extra spaces, and numbers, adds macrons\"\"\"\n",
    "        stopwords = []\n",
    "        with open('stopwords_latin.txt') as text_file:\n",
    "            for line in text_file.readlines():\n",
    "                stopwords.append(line)\n",
    "        remove_chars = ['.', ',', ';', ':', '+', '-']\n",
    "        altered_text = self.text\n",
    "        for remove_char in remove_chars:\n",
    "            altered_text = altered_text.replace(remove_char, '')\n",
    "        return LatinText(altered_text.lower()\n",
    "            ).rm_lines(\n",
    "            ).normalize(\n",
    "            ).rm_stopwords(stopwords\n",
    "            ).rm_spaces()\n",
    "    \n",
    "    @property\n",
    "    def text_lemmatized(self):\n",
    "        \"\"\"Gets the clean form of the text then transforms all words to their lemmata.\"\"\"\n",
    "        return self.text_clean.lemmatize()\n",
    "    \n",
    "    @property\n",
    "    def entities(self):\n",
    "        \"\"\"Scans text with cltk's entity recognition and returns a list.\"\"\"\n",
    "        return LatinText(self.text).entities()\n",
    "    \n",
    "    def longest_common_substring(self, other_string):\n",
    "        \"\"\"Returns the longest substring that this and another charter share.\"\"\"\n",
    "        return LatinText(self.text).longest_common_substring(other_string)\n",
    "    \n",
    "    def compare_min_hash(self, other_string):\n",
    "        \"\"\"Compares the text minhash similarity of this and another charter.\"\"\"\n",
    "        return LatinText(self.text).compare_min_hash(other_string)\n",
    "        \n",
    "    def word_count(self):\n",
    "        \"\"\"Gives a dictionary, each key is a word appearing and the value is the count.\"\"\"\n",
    "        return LatinText(str(self.text_lemmatized)).word_count()\n",
    "    \n",
    "    def word_count_raw(self):\n",
    "        \"\"\"Same as .word_count(), but does not lemmatize words before counting.\"\"\"\n",
    "        return LatinText(self.text).word_count()\n",
    "    \n",
    "    def clausulae_count(self):\n",
    "        \"\"\"Similar to word_count, but instead uses cltk to look for poetic clausulae in prose text.\"\"\"\n",
    "        return LatinText(self.text).clausulae()\n",
    "\n",
    "\n",
    "print('Model created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_charters = []\n",
    "lemmatized_charters = []\n",
    "\n",
    "# open session, read texts from db and store in charters\n",
    "session = sessionmaker(bind=engine)()\n",
    "\n",
    "print('Preparing texts (lemmatizing and cleaning)...', end='')\n",
    "counter = 0\n",
    "for charter_obj in session.query(Charter):\n",
    "    counter += 1\n",
    "    cleaned_charters.append(Text(charter_obj.text_clean.tokenize()))\n",
    "    lemmatized_charters.append(Text(charter_obj.text_lemmatized.tokenize()))\n",
    "    if counter % 10 == 0:\n",
    "        print('.', end='')\n",
    "session.close()\n",
    "print('Done!')\n",
    "\n",
    "# converts charters from list of texts into text collection\n",
    "cleaned_charters = TextCollection(cleaned_charters)\n",
    "lemmatized_charters = TextCollection(lemmatized_charters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatized_charters.dispersion_plot(['iesu', 'christi', 'rex', 'deus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iesu')\n",
    "print('Frequency to Document Ratio {}%'.format(round(lemmatized_charters.idf('iesu') * 100, 2)))\n",
    "\n",
    "print('Concordance: First 10 appearances')\n",
    "for concordance_appearance in lemmatized_charters.concordance_list('iesu')[0:10]:\n",
    "    print(concordance_appearance.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Common Contexts')\n",
    "print('\\n---\\nIesu:')\n",
    "print(lemmatized_charters.common_contexts(['iesu']))\n",
    "print('\\n---\\nRex:')\n",
    "print(lemmatized_charters.common_contexts(['rex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_charters.plot(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORE COMING SOON\n",
    "\n",
    "For now, try out the network analysis module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
