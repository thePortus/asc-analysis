{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and Storing\n",
    "\n",
    "[David J. Thomas](mailto:dave.a.base@gmail.com), [thePort.us](http://thePort.us)<br />\n",
    "Instructor of Ancient History and Digital Humanities,<br />\n",
    "Department of History,<br />\n",
    "[University of South Florida](https://github.com/usf-portal)\n",
    "\n",
    "---\n",
    "\n",
    "## This workbook will...\n",
    "\n",
    "* Create a local db to store the data\n",
    "* Scrape/save charter info from ASC and PASE\n",
    "* Scrape/save witness info from PASE\n",
    "\n",
    "---\n",
    "\n",
    "Together, the [Anglo-Saxon Charter (ASC)](http://aschart.kcl.ac.uk) and the [Prosopography of Anglo-Saxon England (PASE)](http://pase.ac.uk/) databases contain massive amounts of information on individuals and sources of individuals in England from 600-900 AD. These workbooks will show you how to use Python3 to scrape these two databases and explore and analyze the data in new ways using network and text analysis. This exercise will focus on charters, legal documents which framed power, and the people in them.\n",
    "\n",
    "We will use the [requests](https://requests.kennethreitz.org/en/master/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) modules to fetch and parse information, [SQLAlchemy](https://www.sqlalchemy.org/) to store it in a local database, [networkx](https://networkx.github.io) to perform network analysis, and finally the [nltk](http://nltk.org) and [cltk](https://cltk.readthedocs.io) modules to do some text analysis on the documents themselves.\n",
    "\n",
    "![Banner image](../assets/network-banner.png)\n",
    "\n",
    "*A network of nearly 2,500 Anglo-Saxons, connected by appearing together as witnesses in nearly 500 medieval documents.*\n",
    "\n",
    "---\n",
    "\n",
    "Medieval charters were documents of importance and power. They bound institutions and powerful individuals together and created links that were displayed to wider communities. These charters were usually grants of land, often from a king, to a monastery, individual, or some other institution. These documents are not just passive windows into the past, scholars from [Geoffrey Koziol](https://history.berkeley.edu/geoffrey-koziol) to [Phillip Buc](https://ias.ceu.edu/people/philippe-buc) all recognize that the individuals who wrote them intended them to be persuasive and ideological documents. Even the use of minor honoriffic formulae (e.g. *gratia dei*, \"By the grace of god\") subtlely, but intentionally, linked the different rulers who used them. Historians generally are far more aware of the performative nature of aristocracy. No single title or claim made one an aristocrat so much as constellations of titles and connections with other elites. More importantly, all these claims and connections were constantly advertised and ritually performed (c.f. [Cheyette, Ermengard of Narbonne and the World of the Troubadours](https://www.amazon.com/Ermengard-Narbonne-Troubadours-Conjunctions-Religion/dp/0801489253); [Althoff, Family, Friends and Followers: Political and Social Bonds in Early Medieval Europe](https://www.amazon.com/Family-Friends-Followers-Political-Cambridge/dp/0521779340)). So, traditional historical research suggests these documents were important, and their relationships warrant investigation.\n",
    "\n",
    "These charters were critical documents in the distribution of land and goods, and all contained witnesses whose authority guaranteed the legitimacy of these documents. These witness lists hold the key to exploring *en masse* what traditional historians have only examined *up close*. You can clearly see the lists of witnesses outlined in the image below.\n",
    "\n",
    "---\n",
    "\n",
    "![Image of manuscript outlining witnesses](../assets/charter-large.jpg)\n",
    "\n",
    "*Charter of King Æthelwulf of Wessex to his minister Æthelmod, 843 A.D.* ([Source](https://tenthmedieval.wordpress.com/2014/08/21/before-you-write-a-charter/))\n",
    "\n",
    "Spread over hundreds of documents, the totality of their interconnections reveals much about the Anglo-Saxon elite. We mush be careful, however. These documents were heavily ideological, and appearances together in a legal document do not necessarily denote a meaningful connection in practice. But, if we consider these documents another way, we can learn so much more. Looking at the sum of these connections may not tell us about the *reality* of social connections, but it can tell us worlds about the *image* of social relationships **that they wanted to project to others**.\n",
    "\n",
    "Through the witness lists and the text of the charters themselves, these databases provide an excellent test ground for this approach. They are limited in space and time, allowing us to see change and evolution over time while maintaining a united historical and social context. The datasets are robust, without getting too large to analyze without more powerful tools. Above all, this is a demonstrate of methods and possibilities for other historical investigations.\n",
    "\n",
    "While appearing together hardly means that there is any real connection between two witnesses, it does mean that they were openly connected and that their social authority was indirectly bound toegether. One goal of these workbooks is to unravel the connections and individuals bound up in these charters. Another goal is also to show you what can be done with an existing data source if you explore it in a new way.\n",
    "\n",
    "* To technically minded people, I hope to show you how to apply these methods to historical topics.\n",
    "* To historians, I hope to show you how current historiographical debates directly relate to, and can benefit from, not only the technical possibilities but also the theoretical discussions in various computational methodologies.\n",
    "\n",
    "Let's get started by importing the various dependencies we need.... run the first Python cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Import Module Dependencies\n",
    "\n",
    "The cell below loads all other Python packages needed. These are the packages installed when you ran the `pip install -r requirements.txt`. By using the `import` command, as well as the `from * import *` command, we are bringing the objects and functions inside those modules into active memory, allowing us to utilize them below.\n",
    "\n",
    "\n",
    "You **must** run this before any other cells. Wait until you see `PROCEED` to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to help us create filepaths and open files\n",
    "import os\n",
    "# so we can quickly read and write CSV files\n",
    "import csv\n",
    "# so we can throttle the speed of our web requests\n",
    "import time\n",
    "# requests lets us get the raw HTML of web pages in a single line!\n",
    "import requests\n",
    "# BeautifulSoup then lets us parse that HTML and get data out of it pythonically\n",
    "from bs4 import BeautifulSoup\n",
    "# then we can use sqlalchemy to store all that data \n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "# networkx will let us export the information in a common format (and more in the next module)\n",
    "import networkx as nx\n",
    "# IPython.display is just a utility so we can clear the display between each of the hundreds of webpages we scrape\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# specify the location of the db file and set a delay on the web scraper for good internet citizenship\n",
    "DB_PATH = 'sqlite:///charters.db'\n",
    "SCRAPE_DELAY = 0.5\n",
    "\n",
    "print('Modules imported! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2) Create Database/Schema\n",
    "\n",
    "Before we try to scrape data from the website, we need to have a place to store it. Rather than export the data as a spreadsheet, storing it as a local database will allow us to perform far more powerful kinds of analysis in later steps. In addition, using the database will allow us to easily export our information in a variety of formats.\n",
    "\n",
    "We are going to use a simple type of database, sqlite. In order to simplify interaction with the database, we are going to use the Python package [SQLAlchemy](https://www.sqlalchemy.org/). This will allow us to easily get related bits of data in a 'Pythonic' way. For example, to get all the people appearing on a charter named `charter` you would write `charter.people`, which will give you a list populated with the relevant items.\n",
    "\n",
    "The code below first defines the database, and also three 'models'... `Charter`, `Person`, and a third table which will store the relational information (which people appeared on which charters). You can envision the way the data for each these models are stored as something like a spreadsheet. The `Person` model is actually stored in a table named `people`, which has 3 columns.\n",
    "\n",
    "After defining these models inside Python using [SQLAlchemy](https://www.sqlalchemy.org/), the last line of code actually commits these changes to the database, which should now have three empty tables, named `charters`, `people`, and `charter_witnesses`. If you want to manually examine the database, you can use a free program like [SQLite Browser](http://sqlitebrowser.org/).\n",
    "\n",
    "I used an entity-relationship modeler to create a diagram of the different tables below, their columns (properties), and which columns link everything together. While charters and people are stored in separate people, it is the third table which allows us to connect them together. This table contains only the IDs of people and the IDs of the charters on which they appear. The diagram below simply visually illustrates what they code in the following cell is going to create in the database.\n",
    "\n",
    "![EER diagram of our SQLite database](../assets/eer.png)\n",
    "\n",
    "You will see why I chose the fields I did for each table when you look at the pages for [charters](http://www.aschart.kcl.ac.uk/charters/s0002.html) and for [witnesses](http://www.pase.ac.uk/jsp/ASC/factoid.jsp?factoidKey=24083). There is a lot of information we can easily scrape about the charters... but as for the people, we have little more than a list names/unique IDs. It took a little bit for me to work it out, but for right now, take my choices in columns on faith. You will see how we actually scrape it and store parts of the page in each spot in the following steps.\n",
    "\n",
    "*To understand more about SQLAlchemy, I highly recommend the following*... [1](https://docs.sqlalchemy.org/en/13/orm/tutorial.html) | [2](https://www.pythoncentral.io/introductory-tutorial-python-sqlalchemy/) | [3](https://auth0.com/blog/sqlalchemy-orm-tutorial-for-python-developers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the DB engine\n",
    "engine = sql.create_engine(DB_PATH, encoding='utf-8')\n",
    "# create our Base object, each time we want to define a new table, we will simple extend this object\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Charter(Base):\n",
    "    \"\"\"Table for each medieval charter (usually a grant of land from a king to a monastery).\"\"\"\n",
    "    __tablename__ = 'charters'\n",
    "\n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    # modern description of the document\n",
    "    description = sql.Column(sql.String)\n",
    "    # traditional manuscript identification by sawyer, birch, and kemble\n",
    "    sawyer = sql.Column(sql.Integer)\n",
    "    birch = sql.Column(sql.Integer)\n",
    "    kemble = sql.Column(sql.Integer)\n",
    "    # british academy number\n",
    "    british_academy = sql.Column(sql.String)\n",
    "    # published scholarly source\n",
    "    source_used = sql.Column(sql.String)\n",
    "    # current location\n",
    "    archive = sql.Column(sql.String)\n",
    "    # language of the document, Latin, Old English, or bilingual\n",
    "    language = sql.Column(sql.String)\n",
    "    # a single easy date\n",
    "    date = sql.Column(sql.Integer)\n",
    "    # the full text containing more specific date information\n",
    "    scholarly_date = sql.Column(sql.String)\n",
    "    # the low and high dates auto-extracted from the scholarly_date string, and their average\n",
    "    scholarly_date_low = sql.Column(sql.Integer)\n",
    "    scholarly_date_high = sql.Column(sql.Integer)\n",
    "    scholarly_date_avg = sql.Column(sql.Float)\n",
    "    # full text of the charter and any extra notes on the page\n",
    "    text = sql.Column(sql.Text)\n",
    "    notes = sql.Column(sql.Text)\n",
    "    # links to the webpages the charter on asc, pase, and for the charter witnesses on pase\n",
    "    asc_source = sql.Column(sql.String)\n",
    "    pase_source = sql.Column(sql.String)\n",
    "    pase_witnesses = sql.Column(sql.String)\n",
    "    # n2n relational column containing every witness appearing in the document\n",
    "    witnesses = sql.orm.relationship('Person', secondary='charter_witnesses', back_populates='charters')\n",
    "    \n",
    "    @classmethod\n",
    "    def scrape(cls, url):\n",
    "        \"\"\"This is using the BeautifulSoup function we will define below. See there for more on BeautifulSoup\"\"\"\n",
    "        # get portions of page for following steps\n",
    "        content = page_soup.body.div.table.find('td', id='content').div\n",
    "        navbar = content.find('ul', class_='charter-nav')\n",
    "        # get any navigation links\n",
    "        pase_witnesses = navbar.find('li', class_='charter-pase-source').find('a')['href']\n",
    "        # fixes link on asc page, which points to an obsolete address\n",
    "        pase_witnesses = raw_url.replace('ASC', 'Sources').replace('source.jsp', 'DisplaySource.jsp')\n",
    "        \n",
    "    @property\n",
    "    def record(self):\n",
    "        \"\"\"Gets entry data in dictionary format.\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'label': self.id,\n",
    "            'sawyer': self.sawyer,\n",
    "            'birch': self.birch,\n",
    "            'kemble': self.kemble,\n",
    "            'british_academy': self.british_academy,\n",
    "            'source_used': self.source_used,\n",
    "            'archive': self.archive,\n",
    "            'language': self.language,\n",
    "            'date': self.date,\n",
    "            'scholarly_date': self.scholarly_date,\n",
    "            'scholarly_date_low': self.scholarly_date_low,\n",
    "            'scholarly_date_high': self.scholarly_date_high,\n",
    "            'scholarly_date_avg': self.scholarly_date_avg,\n",
    "            'text': self.text,\n",
    "            'notes': self.notes,\n",
    "            'asc_source': self.asc_source,\n",
    "            'pase_source': self.pase_source,\n",
    "            'pase_witnesses': self.pase_witnesses\n",
    "        }\n",
    "    \n",
    "class Person(Base):\n",
    "    \"\"\"Table for each person, usually a witness who appeared in a charter.\"\"\"\n",
    "    __tablename__ = 'people'\n",
    "    \n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    # the short byline of information\n",
    "    description = sql.Column(sql.String)\n",
    "    # link to the person's webpage on pase\n",
    "    link = sql.Column(sql.String)\n",
    "    # n2n relational column containing charters in which the person appears\n",
    "    charters = sql.orm.relationship('Charter', secondary='charter_witnesses', back_populates='witnesses')\n",
    "    \n",
    "    @property\n",
    "    def record(self):\n",
    "        \"\"\"Gets entry data in dictionary format.\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'label': self.id + ': ' + self.description,\n",
    "            'link': self.link\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def earliest_appearance(self):\n",
    "        \"\"\"Returns the date of the earliest charter features said person.\"\"\"\n",
    "        earliest_charter = None\n",
    "        for charter in self.charters:\n",
    "            if not earliest_charter:\n",
    "                earliest_charter = charter.scholarly_date_avg\n",
    "            else:\n",
    "                if charter.scholarly_date_avg < earliest_charter:\n",
    "                    earliest_charter = charter.scholarly_date_avg\n",
    "        return earliest_charter\n",
    "\n",
    "    \n",
    "    def num_coappearances(self, other_person):\n",
    "        \"\"\"Checks the number of times this person appears in the same charters as other_person\"\"\"\n",
    "        total_counter = 0\n",
    "        for charter in self.charters:\n",
    "            for other_charter in other_person.charters:\n",
    "                if charter.id == other_charter.id:\n",
    "                    total_counter += 1\n",
    "        return total_counter\n",
    "    \n",
    "    \n",
    "class CharterWitness(Base):\n",
    "    \"\"\"Relational n2n table connecting people to the charters in which they were witnesses\"\"\"\n",
    "    __tablename__ = 'charter_witnesses'\n",
    "    # ids of primary keys of each charter and person\n",
    "    charter_id = sql.Column(sql.String, sql.ForeignKey('charters.id'), primary_key=True) \n",
    "    person_id = sql.Column(sql.String, sql.ForeignKey('people.id'), primary_key=True)\n",
    "    # specifies if person is a witness, benefactor, grantee, et.c.\n",
    "    role = sql.Column(sql.String)\n",
    "    # url to the pase webpage of related people, events, et.c.\n",
    "    link = sql.Column(sql.String)\n",
    "    \n",
    "    @property\n",
    "    def record(self):\n",
    "        \"\"\"Gets entry data in dictionary format.\"\"\"\n",
    "        return {\n",
    "            'charter_id': self.charter_id,\n",
    "            'person_id': self.person_id,\n",
    "            'label': self.role,\n",
    "            'link': self.link\n",
    "        }\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "print('Database Configured Successfully! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) Scrape ASC and PASE for Charter Info\n",
    "\n",
    "Our first step will be to get the urls for every charter in the ASC database. Then, using `ASCCharterPage` each page of the ASC database will be requested and parsed into a [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) object.\n",
    "\n",
    "As each ASC page is scraped, the link to charter on the PASE database will be used to instantiate a corresponding `PASECharterPage` object. This object will then be used to grab further information about the charter not located on the ASC page. We will also grab the link to people appearing on the charter from each ASC page which we will use in the following step.\n",
    "\n",
    "*To understand more about requests, I highly recommend the following*... [1](https://www.w3schools.com/python/module_requests.asp) | [2](https://scotch.io/tutorials/getting-started-with-python-requests-get-requests) | [3](https://www.pythonforbeginners.com/requests/using-requests-in-python)\n",
    "\n",
    "*To understand more about BeautifulSoup, I highly recommend the following*... [1](https://beautiful-soup-4.readthedocs.io/en/latest/) | [2](https://www.pythonforbeginners.com/beautifulsoup/beautifulsoup-4-python) | [3](https://www.datacamp.com/community/tutorials/scraping-reddit-python-scrapy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3a) Make Function to Get HTML and Parse into BeautifulSoup\n",
    "\n",
    "In order to speed up other objects and avoid repeating code, we are going to create a function that will do all the work of getting a url, getting webpage HTML, and then converting it into a BeautifulSoup object. This makes it easy to scrape for information in a Pythonic way. The function gets a URL, if there is an error it calls itself recursively until a retry limit is reached.\n",
    "\n",
    "We are defining this function here so that the different objects we will define below to scrape different pages can all use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_page(url, tries_left = 5):\n",
    "    \"\"\"Receives URL, requests raw html, then returns converted BeautifulSoup object.\"\"\"\n",
    "    # declare variable for raw html\n",
    "    page_html = None\n",
    "    # ensure tries_left is set and valid, if not set to 5, check if url is valid\n",
    "    if not tries_left or type(tries_left) != int or tries_left < 0:\n",
    "        tries_left = 5\n",
    "    if type(url) != str:\n",
    "        raise Exception('URL must be a valid string')\n",
    "    # enforce a time delay between each scrape for good internet citizenship\n",
    "    time.sleep(SCRAPE_DELAY)\n",
    "    print('Getting', url)\n",
    "    # attempt to get page data, decrement tries_left if successful\n",
    "    try:\n",
    "        page_html = requests.get(url).text\n",
    "        tries_left -= 1\n",
    "    # if an error occured, retry by returning recursively\n",
    "    except:\n",
    "        print('Error getting', url)\n",
    "        if tries_left > 0:\n",
    "            print('Retrying...')\n",
    "            return soup_page(url, tries_left=triest_left-1)\n",
    "        if tries_left <= 0:\n",
    "            print('Retry limit reached, ABORTING parse of', url)\n",
    "            return None\n",
    "    print('Success, souping...')\n",
    "    # if all went well, return new BeautifulSoup populated with the page html and parser set to html.parser\n",
    "    return BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "print('Function defined! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3b) Define Object to Scrape Info from a ASC Charter Page\n",
    "\n",
    "Now that we have a function which can give us a parser for any page, we can create an object that will parse and quickly give the data for any *one* ASC charter page. We will use this definition to create a bunch of instances to extract the data from the ASC pages. First through, we need to define the object, and which parts of the page should be tied to which properties and methods.\n",
    "\n",
    "If we look at the [webpage for a single charter](http://www.aschart.kcl.ac.uk/charters/s0002.html), it looks like this (orange border highlighting is mine)...\n",
    "\n",
    "![Web page for charter S2](../assets/asc-charter.png)\n",
    "\n",
    "So, we want to extract the text of the charter, but we also want to deal with several other things, indicated by the highlighting. Because we will want to go back later and get information about who was on which inscription, we want to get url of the \"List of Witnesses in PASE\". This will speed up our scraping later.\n",
    "\n",
    "You can see that there is also information about its location, dating, and an overall description. We will want to store that. In addition, we will want to devise a way to parse out numerical date information reliably from the text. Since there is some variation on how it appears from charter to charter, we will have to deal with that by defining a method.\n",
    "\n",
    "In addition, in the text itself are these pesky flags that say things like `INVOCATION`, `PROEM`, or `DISPOSITIVE WORD`. These indicate the different roles that different parts of the text are playing. This could be a very interesting to expand the linguistic angle of this exercise in the future... but for our purposes here, we need to filter them out so that we have the language of the charter in plain text.\n",
    "\n",
    "So, in order to deal with all this, we are now going to define an object that can handle extracting the data from any given charter page. We won't actually use this object until just a little later, but let's go ahead and define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASCCharterPage:\n",
    "    \"\"\"Extracts data from a single charter from the ASC database.\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        \"\"\"Gets url and pre-parses the html\"\"\"\n",
    "        self._url = url\n",
    "        # fetch page and parse into beautifulsoup object\n",
    "        page_soup = soup_page(url)\n",
    "        # get only portion of page with charter-specific content\n",
    "        self.data = page_soup.body.div.table.find('td', id='content').div\n",
    "        # eager load navbar to speed up link retreival\n",
    "        self._navbar = self.data.find('ul', class_='charter-nav')\n",
    "            \n",
    "    @property\n",
    "    def id(self):\n",
    "        \"\"\"Gives the charter ID, with spaces removed.\"\"\"\n",
    "        return self.data.div.div.h1.get_text().replace(' ', '')\n",
    "        \n",
    "    @property\n",
    "    def pase_source(self):\n",
    "        \"\"\"Url to source in PASE database, if extant, otherwise None.\"\"\"\n",
    "        raw_url = self._navbar.find('li', class_='charter-pase-source').find('a')['href']\n",
    "        # fixes link on asc page, which points to an obsolete address\n",
    "        return raw_url.replace('ASC', 'Sources').replace('source.jsp', 'DisplaySource.jsp')\n",
    "        \n",
    "    @property\n",
    "    def pase_witnesses(self):\n",
    "        \"\"\"URL to list of people appearing on charter in the PASE database.\"\"\"\n",
    "        try:\n",
    "            return self._navbar.find('li', class_='charter-pase-witnesses').find('a')['href']\n",
    "        # return None if no witnesses are found (i.e. no link exists)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def description(self):\n",
    "        \"\"\"Modern description of charter.\"\"\"\n",
    "        return self.data.p.get_text()\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        \"\"\"Full text of the charter, in original language. Editorial clause markings removed.\"\"\"\n",
    "        # grab text and convert from latin-1 to utf-8 encoding\n",
    "        raw_text = self.data.find_all('div')[3].get_text()\n",
    "        clean_text = bytearray(raw_text, 'latin-1').decode('utf-8')\n",
    "        # remove text of embedded editorial marks\n",
    "        remove_phrases = [\n",
    "            'DATING CLAUSE', 'INVOCATION', 'PROMULGATION PLACE', 'CURSE',\n",
    "            'DISPOSITIVE WORD', 'BOUNDS', 'PROEM',\n",
    "        ]\n",
    "        for remove_phrase in remove_phrases:\n",
    "            clean_text = clean_text.replace(remove_phrase, '')\n",
    "        # removes extra whitespace by spliting into list of words and rejoining\n",
    "        return clean_text\n",
    "    \n",
    "print('ASC Charter Scraper Defined! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3c) Define Object to Scrape Info from a PASE Charter Page\n",
    "\n",
    "Just as we did above for individual ASC charter pages, we need a different object which can rapidly scrape information from each PASE database page related to a charter. By scraping different information from each database about the same charter, we are able to round out our picture of not only the charter but the people on it.\n",
    "\n",
    "We we look at a [charter webpage from the PASE database](http://www.pase.ac.uk/jsp/Sources/DisplaySource.jsp?sourceKey=107), we can see it contains more information.\n",
    "\n",
    "![Page of a charter from PASE](../assets/pase-charter.png)\n",
    "\n",
    "Again, we are going to define another object which utilizes `soup_page` to target a specific page. This time we can extract more full dating information about each charter... including a simple numerical date, a text description of dating & reasons, as well as a possible date range. This allows us to filter and explore chronological information (when extant) with more nuance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASECharterPage:\n",
    "    \"\"\"Extracts further information about charters from their PASE page.\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        # fetch page and convert to beautifulsoup object\n",
    "        self.data = soup_page(url)\n",
    "        sections = self.data.find_all('div', class_='t01')\n",
    "        self._charter_info = sections[0].table.tr.td.table.find_all('tr')\n",
    "        self._source_info = sections[1].table.tr.td.table.find_all('tr')\n",
    "    \n",
    "    @property\n",
    "    def sawyer(self):\n",
    "        \"\"\"Returns the sawyer number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def birch(self):\n",
    "        \"\"\"Returns the birch number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def kemble(self):\n",
    "        \"\"\"Returns the kemble number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[2].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def british_academy(self):\n",
    "        \"\"\"Returns the British Academy reference, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[3].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def source_used(self):\n",
    "        \"\"\"Gives modern source used.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[4].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def archive(self):\n",
    "        \"\"\"Gives name of modern archive housing the charter.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[5].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def language(self):\n",
    "        \"\"\"Gives language(s) used in charter.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def date(self):\n",
    "        \"\"\"Gives long-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def scholarly_date(self):\n",
    "        \"\"\"Gives short-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[2].td.get_text()\n",
    "            \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_low(self):\n",
    "        \"\"\"Will return low date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[0].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_high(self):\n",
    "        \"\"\"Will return high date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[1].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_avg(self):\n",
    "        \"\"\"Returns mean date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        low_date = self.scholarly_date_low\n",
    "        high_date = self.scholarly_date_high\n",
    "        if low_date == high_date:\n",
    "            return low_date\n",
    "        else:\n",
    "            return round((low_date + high_date) / 2, 2)\n",
    "    \n",
    "    @property\n",
    "    def notes(self):\n",
    "        \"\"\"Gives miscellaneous notes on charter.\"\"\"\n",
    "        try:\n",
    "            return self.data.find('div', class_='rec').p.get_text()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "print('PASE Charter Scraper Defined! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3d) Scrape Charter Info from ASC and PASE\n",
    "\n",
    "Now that we have both scrapers defined, we can put them to use! Time to download the information and store it in our database. We will use the two scraper objects we created in steps 3b & 3c to get information off of the website and then store it our local database using the models we defined in step 2.\n",
    "\n",
    "To do this, we must first go to the main \"browse\" page of the database, scrape it to get the URL of each charter, and then traverse the list. For the ASC database, the best page to do this is the [Index by Sawyer](http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html). Which you can see below.\n",
    "\n",
    "![ASC Index by Sawyer webpage](../assets/asc-by-sawyer.png)\n",
    "\n",
    "*This is where it finally goes into practice!*\n",
    "\n",
    "1. We want to use our `soup_page` function to fetch & parse the main page.\n",
    "2. We want to extract the links from each of the sawyers, highlighted in orange above\n",
    "3. We want to create ASC scraper objects with each of the links and extract data from each page\n",
    "4. We want to create PASE scraper objects with the urls from each ASC page, and extract the PASE data\n",
    "5. Store all the information we scrape with both into the database we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to open a session with the local database\n",
    "session = sessionmaker(bind=engine)()\n",
    "\n",
    "# before scraping information, we need to get the urls for every ASC charter\n",
    "asc_links = []\n",
    "charter_soup = None\n",
    "# fetch page and parse into beautifulsoup object\n",
    "page_soup = soup_page('http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html')\n",
    "# get only portion of page with charter-specific content\n",
    "charter_soup = page_soup.body.div.table.find('tr', class_='r02').find('td', id='content').div\n",
    "# looping through each section and group of rulers\n",
    "for ruler_section in charter_soup.find_all('ul', class_='asc-expand'):\n",
    "    for ruler_group in ruler_section.find_all('li'):\n",
    "        # get relative links from <a> tags append full link to self.data by adding root_url\n",
    "        for charter_link_wrapper in ruler_group.find_all('li'):\n",
    "            # when joining, trim initial two charcters (which are unwanted dots '.')\n",
    "            asc_links.append('http://www.aschart.kcl.ac.uk' + charter_link_wrapper.a['href'][2:])\n",
    "\n",
    "charter_counter = 0\n",
    "# loop through each charter found online\n",
    "for charter_url in asc_links:\n",
    "    clear_output()\n",
    "    print('Gathering charters: {} of {}...'.format(charter_counter + 1, 467))\n",
    "    try:\n",
    "        asc_charter_page = ASCCharterPage(charter_url)\n",
    "        pase_charter_page = PASECharterPage(asc_charter_page.pase_source)\n",
    "        # make new charter and add to session\n",
    "        session.add(Charter(\n",
    "            id=asc_charter_page.id,\n",
    "            description=asc_charter_page.description,\n",
    "            sawyer=pase_charter_page.sawyer,\n",
    "            birch=pase_charter_page.birch,\n",
    "            kemble=pase_charter_page.kemble,\n",
    "            british_academy=pase_charter_page.british_academy,\n",
    "            source_used=pase_charter_page.source_used,\n",
    "            archive=pase_charter_page.archive,\n",
    "            language=pase_charter_page.language,\n",
    "            date=pase_charter_page.date,\n",
    "            scholarly_date=pase_charter_page.scholarly_date,\n",
    "            scholarly_date_low=pase_charter_page.scholarly_date_low,\n",
    "            scholarly_date_high=pase_charter_page.scholarly_date_high,\n",
    "            scholarly_date_avg=pase_charter_page.scholarly_date_avg,\n",
    "            text=asc_charter_page.text,\n",
    "            notes=pase_charter_page.notes,\n",
    "            asc_source=asc_charter_page._url,\n",
    "            pase_source=asc_charter_page.pase_source,\n",
    "            pase_witnesses=asc_charter_page.pase_witnesses\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print('Error loading page at', charter_url, '(skipped)')\n",
    "    charter_counter += 1\n",
    "# commit all changes to the local db\n",
    "session.commit()\n",
    "session.close()\n",
    "\n",
    "print('Charters successfully scraped! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4) Scrape PASE for People in Charters\n",
    "\n",
    "Great, so we have lots of information about the charters, but knowing something about the social context would add a lot to this. So, time to define yet a third scraper. This one is going to go through the PASE database, but this time it is going to seek information about which people appeared in which medieval charters. This particular exercise doesn't scrape much metadata about the people. Nevertheless, each person has a unique ID number. This is going to allow us to connect people together in a network based upon their shared appearances at witnesses.\n",
    "\n",
    "Looking at a [webpage from PASE](http://www.pase.ac.uk/jsp/ASC/factoid.jsp?factoidKey=24083) containing all related people, events, et.c., we can see that the witnesses are clearly listed by their name/unique ID.\n",
    "\n",
    "![Page from PASE containing charter witnesses](../assets/pase-witnesses.png)\n",
    "\n",
    "We are simply going to extract the name/unique ID of each person and the short byline about them, so that will not be difficult. Just like before, we need to define an object to quickly extract the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4a) Define Object to Scrape Info from a PASE Charter Witnesses Page\n",
    "\n",
    "We need to define an object to scrape the page. This one represents a single page from the PASE database containing the list of witnesses that appear on a single charter. There are only as many pages are there are charters, and since we are not going further with metadata about each person, that means we thankfully do not have to extract thousands of individual pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASEWitnesses:\n",
    "    \"\"\"Gets basic information about people on the charter from a PASE witnesses page\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self._url = url\n",
    "        self._soup = soup_page(self._url)\n",
    "        # get only portion of page with relevant data\n",
    "        try:\n",
    "            self.data = self._soup.find('div', class_='rec').find('ul').find_all('li')\n",
    "        # sometimes notes preceed data, in which case get second div with class rec\n",
    "        except:\n",
    "            self.data = self._soup.find_all('div', class_='rec')[1].find('ul').find_all('li')\n",
    "    @property\n",
    "    def witnesses(self):\n",
    "        witness_list = []\n",
    "        for witness_entry in self.data:\n",
    "            witness_link_element = witness_entry.find('a')\n",
    "            try:\n",
    "                witness_role = witness_entry.find('strong').get_text()\n",
    "            except:\n",
    "                witness_role = 'Witness'\n",
    "            witness_list.append({\n",
    "                    'role': witness_role,\n",
    "                    'name': witness_link_element.get_text().lstrip(),\n",
    "                    'link': witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                    'description': witness_entry.find('em').get_text()\n",
    "                })\n",
    "            # look for nested witnesses, sometimes buried in recursive em tags\n",
    "            nested_witnesses_element = None\n",
    "            try:\n",
    "                nested_witnesses_element = self._soup.find('div', class_='rec').find('ul').find('em')\n",
    "            except:\n",
    "                nested_witnesses_element = self._soup.find_all('div', class_='rec')[1].find('ul').find('em')\n",
    "            while nested_witnesses_element is not None:\n",
    "                nested_witnesses = nested_witnesses_element.find_all('li')\n",
    "                for nested_witness_entry in nested_witnesses:\n",
    "                    nested_witness_link_element = witness_entry.find('a')\n",
    "                    try:\n",
    "                        nested_witness_role = nested_witness_entry.find('strong').get_text()\n",
    "                    except:\n",
    "                        nested_witness_role = 'Witness'\n",
    "                    witness_list.append({\n",
    "                        'role': nested_witness_role,\n",
    "                        'name': nested_witness_link_element.get_text(),\n",
    "                        'link': nested_witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                        'description': nested_witness_entry.find('em').get_text()\n",
    "                    })\n",
    "                nested_witnesses_element = nested_witnesses_element.find('em')\n",
    "        return witness_list\n",
    "    \n",
    "\n",
    "print('PASE Witnesses Scraper Defined! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) Scrape Witness Info\n",
    "\n",
    "As before, we will instantiate our scraper, creating one for each charter. Traversing the list, we will store the information in the Witness and CharterWitness tables. This will allow us to get network information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session(autoflush=False)\n",
    "\n",
    "charter_counter = 0\n",
    "# now we want to query our local db for every charter, which is returned as a Charter object\n",
    "for charter in session.query(Charter):\n",
    "    clear_output()\n",
    "    print('Witnesses found in {} charters (so far) of {}...'.format(charter_counter + 1, 467))\n",
    "    # then get the link to the corresponding PASE page with witness information, skip if there is no link\n",
    "    witnesses_link = charter.pase_witnesses\n",
    "    if witnesses_link is not None:\n",
    "        # use the scraper to get witness data as list of dicts, only proceed if results were found\n",
    "        witness_list = PASEWitnesses(witnesses_link).witnesses\n",
    "        # loop through the list of witnesses\n",
    "        for witness in witness_list:\n",
    "            # query to see if person already exists in db, if no results found, then add them\n",
    "            person_query = session.query(Person).filter(Person.id == witness['name'])\n",
    "            # if any results are in the list, it will set person_found to True\n",
    "            person_found = False\n",
    "            for person in person_query:\n",
    "                person_found = True\n",
    "            if not person_found:\n",
    "                try:\n",
    "                    session.add(Person(\n",
    "                        id=witness['name'],\n",
    "                        description=witness['description'],\n",
    "                        link=witness['link']\n",
    "                    ))\n",
    "                    print('Added person {}'.format(witness['name']))\n",
    "                    session.commit()\n",
    "                except:\n",
    "                    session.rollback()\n",
    "            # add charter/person relationship information to `charter_witnesses` table created above\n",
    "            try:\n",
    "                session.add(CharterWitness(\n",
    "                    charter_id=charter.id,\n",
    "                    person_id=witness['name'],\n",
    "                    role=witness['role'],\n",
    "                    link=str(witnesses_link)\n",
    "                ))\n",
    "                session.commit()\n",
    "            except:\n",
    "                session.rollback()\n",
    "            print('Added person/charter relationship {} -> {}'.format(witness['name'], charter.id))\n",
    "    charter_counter += 1\n",
    "    \n",
    "# commit all changes to the local db\n",
    "session.close()\n",
    "\n",
    "print('Witnesses successfully scraped! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Preview Results\n",
    "\n",
    "Let's see if it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session(autoflush=False)\n",
    "\n",
    "charters = session.query(Charter)\n",
    "\n",
    "charter_counter = 0\n",
    "for charter in charters:\n",
    "    charter_counter += 1\n",
    "    \n",
    "print('{} total charters scraped.'.format(charter_counter))\n",
    "print('Preview of first charter...')\n",
    "print(charters[0].text.replace('\\n', ''))\n",
    "\n",
    "print('Preview successfull! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Export Data\n",
    "\n",
    "This portion is optional, but allows you to immediately export your results for other platforms. All data will be saved to the `/data` directory. If you are using this on Binder, you can navigate there and download the data to your local device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a) Charters as Plain Text Files\n",
    "\n",
    "Some text analysis programs like to use plain .txt files. This cell will generate a folder full of .txt files, one for each charter. Works well with [Voyant](https://voyant-tools.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exporting charters as a folder of .txt files')\n",
    "\n",
    "session = sessionmaker(bind=engine)()\n",
    "charters = session.query(Charter)\n",
    "session.close()\n",
    "\n",
    "export_folder_path = os.path.join('../data', 'raw_charter_txts')\n",
    "# ensure parent directory exists, if not, make it\n",
    "if not os.path.exists(export_folder_path):\n",
    "    os.makedirs(export_folder_path)\n",
    "\n",
    "for charter in charters:\n",
    "    output_path = os.path.join(export_folder_path, charter.id + '.txt')\n",
    "    with open(output_path, mode='w+') as txtfile:\n",
    "        txtfile.write(charter.text)\n",
    "        \n",
    "print('Successfully exported to data/raw_charter_txts/! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Charters as CSV Files\n",
    "\n",
    "A more standard format are CSV files. This cell generates one .csv file, with a column for every metadata field. This works well for the majority of text analysis programs and the ability to connect metadata expands the possible levels of analysis. Works well with programs like [Overview](https://www.overviewdocs.com/) or [Orange3](https://orange.biolab.si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exporting charters as a .csv file')\n",
    "\n",
    "session = sessionmaker(bind=engine)()\n",
    "charters = session.query(Charter)\n",
    "session.close()\n",
    "\n",
    "export_path = os.path.join('../data', 'raw_charters.csv')\n",
    "# ensure parent directory exists, if not, make it\n",
    "if not os.path.exists(export_folder_path):\n",
    "    os.makedirs(export_folder_path)\n",
    "\n",
    "fieldnames = [\n",
    "    'id', 'description', 'sawyer', 'birch', 'kemble', 'british_academy', 'source_used', 'archive',\n",
    "    'language', 'date', 'scholarly_date', 'scholarly_date_low', 'scholarly_date_high',\n",
    "    'scholarly_date_avg', 'text', 'notes', 'asc_source', 'pase_source', 'pase_witnesses'\n",
    "]\n",
    "\n",
    "# open the csv file for writing\n",
    "with open(export_path, mode='w+', encoding='utf-8') as csvfile:\n",
    "    # create the csv writer for the file and write the header row\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    # loop through charters\n",
    "    for charter in charters:\n",
    "        new_record = charter.record\n",
    "        # move the 'label' property to the 'description' property\n",
    "        new_record['description'] = new_record['label']\n",
    "        del(new_record['label'])\n",
    "        # write new csv row\n",
    "        writer.writerow(new_record)\n",
    "        \n",
    "print('Successfully exported to data/raw_charters.csv! PROCEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c) Coappearances of Witnesses as Network File\n",
    "\n",
    "Now that we have information about charters, and witnesses, and their relationships, we can use the networkx package to easily create a .gexf file, the most commonly used network graph format. It works very well with [Gephi](https://gephi.org/). \n",
    "\n",
    "**This step is CRITICAL if you want to do the next workbook!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sessionmaker(bind=engine)()\n",
    "witnesses = session.query(Person)\n",
    "session.close()\n",
    "\n",
    "print('Building network graph...', end='')\n",
    "# create an empty networkx graph\n",
    "witness_network = nx.Graph(\n",
    "    label='',\n",
    "    link=None,\n",
    "    weight=1,\n",
    "    type='Undirected',\n",
    "    community=None,\n",
    "    degree=0,\n",
    "    degree_centrality=0,\n",
    "    betweeness_centrality=0,\n",
    "    eigenvector_centrality=0,\n",
    "    closeness_centrality=0,\n",
    "    harmonic_centrality=0,\n",
    ")\n",
    "\n",
    "# populate the nodes with each witness record\n",
    "for person in witnesses:\n",
    "    witness_network.add_node(person.id, **person.record)\n",
    "\n",
    "counter = 0\n",
    "# to build edges, we need to compare every witness against each other for the number of times they appear\n",
    "for index, person in enumerate(witnesses):\n",
    "    for other_person in witnesses[index + 1:]:\n",
    "        counter += 1\n",
    "        if counter % 50000 == 0:\n",
    "            print('.', end='')\n",
    "        num_coappearances = person.num_coappearances(other_person)\n",
    "        # only add an edge if they actually appeared together\n",
    "        if num_coappearances > 0:\n",
    "            witness_network.add_edge(\n",
    "                person.id,\n",
    "                other_person.id,\n",
    "                label='{} -> {}'.format(person.id, other_person.id),\n",
    "                weight=person.num_coappearances(other_person),\n",
    "                type='Undirected'\n",
    "            )\n",
    "\n",
    "print(' Done!\\n\\nExporting network to `data/witness_network.gexf`...')\n",
    "nx.write_gexf(witness_network, '../data/witness_network.gexf')\n",
    "print('Exported network successfully! PROCEED TO NEXT WORKBOOK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Proceed to the next workbook!\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
