{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and Storing\n",
    "\n",
    "[David J. Thomas](mailto:dave.a.base@gmail.com), [thePortus.com](http://thePortus.com)<br />\n",
    "Instructor of Ancient History and Digital Humanities,<br />\n",
    "Department of History,<br />\n",
    "[University of South Florida](https://github.com/usf-portal)\n",
    "\n",
    "---\n",
    "\n",
    "## This workbook will...\n",
    "\n",
    "* Create a local db to store the data\n",
    "* Scrape/save charter info from ASC and PASE\n",
    "* Scrape/save witness info from PASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Import Module Dependencies\n",
    "\n",
    "The cell below loads all other Python packages needed. You **must** run this before any other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from IPython.display import clear_output\n",
    "\n",
    "DB_PATH = 'sqlite://'\n",
    "SCRAPE_DELAY = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create Database/Schema\n",
    "\n",
    "Before we try to scrape data from the website, we need to have a place to store it. Rather than export the data as a spreadsheet, storing it as a local database will allow us to perform far more powerful kinds of analysis in later steps. In addition, using the database will allow us to easily export our information in a variety of formats.\n",
    "\n",
    "We are going to use a simple type of database, sqlite. In order to simplify interaction with the database, we are going to use the Python package [SQLAlchemy](https://www.sqlalchemy.org/). This will allow us to easily get related bits of data in a 'Pythonic' way. For example, to get all the people appearing on a charter named `charter` you would write `charter.people`, which will give you a list populated with the relevant items.\n",
    "\n",
    "The code below first defines the database, and also three 'models'... `Charter`, `Person`, and a third table which will store the relational information (which people appeared on which charters). You can envision the way the data for each these models are stored as something like a spreadsheet. The `Person` model is actually stored in a table named `people`, which has 3 columns.\n",
    "\n",
    "After defining these models inside Python using [SQLAlchemy](https://www.sqlalchemy.org/), the last line of code actually commits these changes to the database, which should now have three empty tables, named `charters`, `people`, and `charter_witnesses`. If you want to manually examine the database, you can use a free program like [SQLite Browser](http://sqlitebrowser.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Configured Successfully\n"
     ]
    }
   ],
   "source": [
    "engine = sql.create_engine(DB_PATH, encoding='utf-8')\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Charter(Base):\n",
    "    __tablename__ = 'charters'\n",
    "\n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    sawyer = sql.Column(sql.Integer)\n",
    "    birch = sql.Column(sql.Integer)\n",
    "    kemble = sql.Column(sql.Integer)\n",
    "    british_academy = sql.Column(sql.String)\n",
    "    source_used = sql.Column(sql.String)\n",
    "    archive = sql.Column(sql.String)\n",
    "    language = sql.Column(sql.String)\n",
    "    date = sql.Column(sql.Integer)\n",
    "    scholarly_date = sql.Column(sql.String)\n",
    "    scholarly_date_low = sql.Column(sql.Integer)\n",
    "    scholarly_date_high = sql.Column(sql.Integer)\n",
    "    scholarly_date_avg = sql.Column(sql.Float)\n",
    "    text = sql.Column(sql.Text)\n",
    "    notes = sql.Column(sql.Text)\n",
    "    asc_source = sql.Column(sql.String)\n",
    "    pase_source = sql.Column(sql.String)\n",
    "    pase_witnesses = sql.Column(sql.String)\n",
    "    \n",
    "    witnesses = sql.orm.relationship('Person', secondary='charter_witnesses', back_populates='charters')\n",
    "    \n",
    "    @classmethod\n",
    "    def scrape(cls, url):\n",
    "        # get portions of page for following steps\n",
    "        content = page_soup.body.div.table.find('td', id='content').div\n",
    "        navbar = content.find('ul', class_='charter-nav')\n",
    "        # get various properties from the page\n",
    "        \n",
    "        # get any navigation links\n",
    "        pase_witnesses = navbar.find('li', class_='charter-pase-source').find('a')['href']\n",
    "        # fixes link on asc page, which points to an obsolete address\n",
    "        pase_witnesses = raw_url.replace('ASC', 'Sources').replace('source.jsp', 'DisplaySource.jsp')\n",
    "    \n",
    "class Person(Base):\n",
    "    __tablename__ = 'people'\n",
    "    \n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    link = sql.Column(sql.String)\n",
    "    \n",
    "    charters = sql.orm.relationship('Charter', secondary='charter_witnesses', back_populates='witnesses')\n",
    "    \n",
    "    @property\n",
    "    def earliest_appearance(self):\n",
    "        \"\"\"Returns the date of the earliest charter features said person.\"\"\"\n",
    "        earliest_charter = None\n",
    "        for charter in self.charters:\n",
    "            if not earliest_charter:\n",
    "                earliest_charter = charter.scholarly_date_avg\n",
    "            else:\n",
    "                if charter.scholarly_date_avg < earliest_charter:\n",
    "                    earliest_charter = charter.scholarly_date_avg\n",
    "        return earliest_charter\n",
    "    \n",
    "    \n",
    "class CharterWitness(Base):\n",
    "    __tablename__ = 'charter_witnesses'\n",
    "    charter_id = sql.Column(sql.String, sql.ForeignKey('charters.id'), primary_key=True) \n",
    "    person_id = sql.Column(sql.String, sql.ForeignKey('people.id'), primary_key=True)\n",
    "    role = sql.Column(sql.String)\n",
    "    link = sql.Column(sql.String)\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "print('Database Configured Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Scrape ASC and PASE for Charter Info\n",
    "\n",
    "Our first step will be to get the urls for every charter in the ASC database. Then, using `ASCCharterPage` each page of the ASC database will be requested and parsed into a [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) object.\n",
    "\n",
    "As each ASC page is scraped, the link to charter on the PASE database will be used to instantiate a corresponding `PASECharterPage` object. This object will then be used to grab further information about the charter not located on the ASC page. We will also grab the link to people appearing on the charter from each ASC page which we will use in the following step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Make Function to Get HTML and Parse into BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined.\n"
     ]
    }
   ],
   "source": [
    "def soup_page(url):\n",
    "    page_html = None\n",
    "    if type(url) != str:\n",
    "        raise Exception('URL must be a valid string')\n",
    "    # enforce a time delay between each scrape\n",
    "    time.sleep(SCRAPE_DELAY)\n",
    "    # attempt to get page data\n",
    "    try:\n",
    "        page_html = requests.get(url).text\n",
    "    # if an error occured, retry by returning recursively\n",
    "    except:\n",
    "        return soup_page(url)\n",
    "    # if all went well, create new BeautifulSoup with page html\n",
    "    return BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "print('Function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Define Object to Scrape Info from a ASC Charter Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASC Charter Scraper Defined\n"
     ]
    }
   ],
   "source": [
    "class ASCCharterPage:\n",
    "    \"\"\"Extracts data from a single charter from the ASC database.\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self._url = url\n",
    "        # fetch page and parse into beautifulsoup object\n",
    "        page_soup = soup_page(url)\n",
    "        # get only portion of page with charter-specific content\n",
    "        self.data = page_soup.body.div.table.find('td', id='content').div\n",
    "        # eager load navbar to speed up link retreival\n",
    "        self._navbar = self.data.find('ul', class_='charter-nav')\n",
    "            \n",
    "    @property\n",
    "    def id(self):\n",
    "        \"\"\"Gives the charter ID, with spaces removed.\"\"\"\n",
    "        return self.data.div.div.h1.get_text().replace(' ', '')\n",
    "        \n",
    "    @property\n",
    "    def pase_source(self):\n",
    "        \"\"\"Url to source in PASE database, if extant, otherwise None.\"\"\"\n",
    "        raw_url = self._navbar.find('li', class_='charter-pase-source').find('a')['href']\n",
    "        # fixes link on asc page, which points to an obsolete address\n",
    "        return raw_url.replace('ASC', 'Sources').replace('source.jsp', 'DisplaySource.jsp')\n",
    "        \n",
    "    @property\n",
    "    def pase_witnesses(self):\n",
    "        \"\"\"URL to list of people appearing on charter in the PASE database.\"\"\"\n",
    "        try:\n",
    "            return self._navbar.find('li', class_='charter-pase-witnesses').find('a')['href']\n",
    "        # return None if no witnesses are found (i.e. no link exists)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def description(self):\n",
    "        \"\"\"Modern description of charter.\"\"\"\n",
    "        return self.data.p.get_text()\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        \"\"\"Full text of the charter, in original language. Editorial clause markings removed.\"\"\"\n",
    "        # grab text and convert from latin-1 to utf-8 encoding\n",
    "        raw_text = self.data.find_all('div')[3].get_text()\n",
    "        clean_text = bytearray(raw_text, 'latin-1').decode('utf-8')\n",
    "        # remove text of embedded editorial marks\n",
    "        remove_phrases = [\n",
    "            'DATING CLAUSE', 'INVOCATION', 'PROMULGATION PLACE', 'CURSE',\n",
    "            'DISPOSITIVE WORD', 'BOUNDS', 'PROEM',\n",
    "        ]\n",
    "        for remove_phrase in remove_phrases:\n",
    "            clean_text = clean_text.replace(remove_phrase, '')\n",
    "        # removes extra whitespace by spliting into list of words and rejoining\n",
    "        return clean_text\n",
    "    \n",
    "print('ASC Charter Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Define Object to Scrape Info from a PASE Charter Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASE Charter Scraper Defined\n"
     ]
    }
   ],
   "source": [
    "class PASECharterPage:\n",
    "    \"\"\"Extracts further information about charters from their PASE page.\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        # fetch page and convert to beautifulsoup object\n",
    "        self.data = soup_page(url)\n",
    "        sections = self.data.find_all('div', class_='t01')\n",
    "        self._charter_info = sections[0].table.tr.td.table.find_all('tr')\n",
    "        self._source_info = sections[1].table.tr.td.table.find_all('tr')\n",
    "    \n",
    "    @property\n",
    "    def sawyer(self):\n",
    "        \"\"\"Returns the sawyer number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def birch(self):\n",
    "        \"\"\"Returns the birch number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def kemble(self):\n",
    "        \"\"\"Returns the kemble number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[2].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def british_academy(self):\n",
    "        \"\"\"Returns the British Academy reference, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[3].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def source_used(self):\n",
    "        \"\"\"Gives modern source used.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[4].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def archive(self):\n",
    "        \"\"\"Gives name of modern archive housing the charter.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[5].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def language(self):\n",
    "        \"\"\"Gives language(s) used in charter.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def date(self):\n",
    "        \"\"\"Gives long-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def scholarly_date(self):\n",
    "        \"\"\"Gives short-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[2].td.get_text()\n",
    "            \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_low(self):\n",
    "        \"\"\"Will return low date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[0].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_high(self):\n",
    "        \"\"\"Will return high date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[1].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_avg(self):\n",
    "        \"\"\"Returns mean date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        low_date = self.scholarly_date_low\n",
    "        high_date = self.scholarly_date_high\n",
    "        if low_date == high_date:\n",
    "            return low_date\n",
    "        else:\n",
    "            return round((low_date + high_date) / 2, 2)\n",
    "    \n",
    "    @property\n",
    "    def notes(self):\n",
    "        \"\"\"Gives miscellaneous notes on charter.\"\"\"\n",
    "        try:\n",
    "            return self.data.find('div', class_='rec').p.get_text()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "print('PASE Charter Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Scrape Charter Info from ASC and PASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering charters: 467 of (probably) 467...\n",
      "Error loading page at http://www.aschart.kcl.ac.uk/charters/s1482.html (skipped)\n",
      "Charters successfully scraped\n"
     ]
    }
   ],
   "source": [
    "# first, we need to open a session with the local database\n",
    "session = sessionmaker(bind=engine)()\n",
    "\n",
    "# before scraping information, we need to get the urls for every ASC charter\n",
    "asc_links = []\n",
    "charter_soup = None\n",
    "# fetch page and parse into beautifulsoup object\n",
    "page_soup = soup_page('http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html')\n",
    "# get only portion of page with charter-specific content\n",
    "charter_soup = page_soup.body.div.table.find('tr', class_='r02').find('td', id='content').div\n",
    "# looping through each section and group of rulers\n",
    "for ruler_section in charter_soup.find_all('ul', class_='asc-expand'):\n",
    "    for ruler_group in ruler_section.find_all('li'):\n",
    "        # get relative links from <a> tags append full link to self.data by adding root_url\n",
    "        for charter_link_wrapper in ruler_group.find_all('li'):\n",
    "            asc_links.append('http://www.aschart.kcl.ac.uk' + charter_link_wrapper.a['href'])\n",
    "\n",
    "charter_counter = 0\n",
    "# loop through each charter found online\n",
    "for charter_url in asc_links:\n",
    "    clear_output()\n",
    "    print('Gathering charters: {} of {}...'.format(charter_counter + 1, 467))\n",
    "    try:\n",
    "        asc_charter_page = ASCCharterPage(charter_url)\n",
    "        pase_charter_page = PASECharterPage(asc_charter_page.pase_source)\n",
    "        # make new charter and add to session\n",
    "        session.add(Charter(\n",
    "            id=asc_charter_page.id,\n",
    "            description=asc_charter_page.description,\n",
    "            sawyer=pase_charter_page.sawyer,\n",
    "            birch=pase_charter_page.birch,\n",
    "            kemble=pase_charter_page.kemble,\n",
    "            british_academy=pase_charter_page.british_academy,\n",
    "            source_used=pase_charter_page.source_used,\n",
    "            archive=pase_charter_page.archive,\n",
    "            language=pase_charter_page.language,\n",
    "            date=pase_charter_page.date,\n",
    "            scholarly_date=pase_charter_page.scholarly_date,\n",
    "            scholarly_date_low=pase_charter_page.scholarly_date_low,\n",
    "            scholarly_date_high=pase_charter_page.scholarly_date_high,\n",
    "            scholarly_date_avg=pase_charter_page.scholarly_date_avg,\n",
    "            text=asc_charter_page.text,\n",
    "            notes=pase_charter_page.notes,\n",
    "            asc_source=asc_charter_page._url,\n",
    "            pase_source=asc_charter_page.pase_source,\n",
    "            pase_witnesses=asc_charter_page.pase_witnesses\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print('Error loading page at', charter_url, '(skipped)')\n",
    "    charter_counter += 1\n",
    "# commit all changes to the local db\n",
    "session.commit()\n",
    "session.close()\n",
    "\n",
    "print('Charters successfully scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Scrape PASE for People in Charters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) Define Object to Scrape Info from a PASE Charter Witnesses Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASE Witnesses Scraper Defined\n"
     ]
    }
   ],
   "source": [
    "class PASEWitnesses:\n",
    "    \"\"\"Gets basic information about people on the charter from a PASE witnesses page\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self._url = url\n",
    "        self._soup = soup_page(self._url)\n",
    "        # get only portion of page with relevant data\n",
    "        try:\n",
    "            self.data = self._soup.find('div', class_='rec').find('ul').find_all('li')\n",
    "        # sometimes notes preceed data, in which case get second div with class rec\n",
    "        except:\n",
    "            self.data = self._soup.find_all('div', class_='rec')[1].find('ul').find_all('li')\n",
    "    @property\n",
    "    def witnesses(self):\n",
    "        witness_list = []\n",
    "        for witness_entry in self.data:\n",
    "            witness_link_element = witness_entry.find('a')\n",
    "            try:\n",
    "                witness_role = witness_entry.find('strong').get_text()\n",
    "            except:\n",
    "                witness_role = 'Witness'\n",
    "            witness_list.append({\n",
    "                    'role': witness_role,\n",
    "                    'name': witness_link_element.get_text().lstrip(),\n",
    "                    'link': witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                    'description': witness_entry.find('em').get_text()\n",
    "                })\n",
    "            # look for nested witnesses, sometimes buried in recursive em tags\n",
    "            nested_witnesses_element = None\n",
    "            try:\n",
    "                nested_witnesses_element = self._soup.find('div', class_='rec').find('ul').find('em')\n",
    "            except:\n",
    "                nested_witnesses_element = self._soup.find_all('div', class_='rec')[1].find('ul').find('em')\n",
    "            while nested_witnesses_element is not None:\n",
    "                nested_witnesses = nested_witnesses_element.find_all('li')\n",
    "                for nested_witness_entry in nested_witnesses:\n",
    "                    nested_witness_link_element = witness_entry.find('a')\n",
    "                    try:\n",
    "                        nested_witness_role = nested_witness_entry.find('strong').get_text()\n",
    "                    except:\n",
    "                        nested_witness_role = 'Witness'\n",
    "                    witness_list.append({\n",
    "                        'role': nested_witness_role,\n",
    "                        'name': nested_witness_link_element.get_text(),\n",
    "                        'link': nested_witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                        'description': nested_witness_entry.find('em').get_text()\n",
    "                    })\n",
    "                nested_witnesses_element = nested_witnesses_element.find('em')\n",
    "        return witness_list\n",
    "    \n",
    "\n",
    "print('PASE Witnesses Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) Scrape Witness Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering witnesses from charter 98 of 467...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-65e3085ef72f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwitnesses_link\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# use the scraper to get witness data as list of dicts, only proceed if results were found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mwitness_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPASEWitnesses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwitnesses_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwitnesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# loop through the list of witnesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwitness\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwitness_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-925fdb924eaf>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# get only portion of page with relevant data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c79749a55b07>\u001b[0m in \u001b[0;36msoup_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msoup_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# if all went well, create new BeautifulSoup with page html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Function defined.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             warnings.warn(RuntimeWarning(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/html/parser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/html/parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<!--\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/html/parser.py\u001b[0m in \u001b[0;36mparse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgtpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cdata_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgtpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mhandle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malready_closed_empty_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mhandle_endtag\u001b[0;34m(self, name, nsprefix)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;31m#print \"End tag: \" + name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popToTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mendData\u001b[0;34m(self, containerClass)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;31m# Reset the data collector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# Should we add this string to the tree at all?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session(autoflush=False)\n",
    "\n",
    "charter_counter = 0\n",
    "# now we want to query our local db for every charter, which is returned as a Charter object\n",
    "for charter in session.query(Charter):\n",
    "    clear_output()\n",
    "    print('Gathering witnesses from charter {} of {}...'.format(charter_counter + 1, 467))\n",
    "    # then get the link to the corresponding PASE page with witness information, skip if there is no link\n",
    "    witnesses_link = charter.pase_witnesses\n",
    "    if witnesses_link is not None:\n",
    "        # use the scraper to get witness data as list of dicts, only proceed if results were found\n",
    "        witness_list = PASEWitnesses(witnesses_link).witnesses\n",
    "        # loop through the list of witnesses\n",
    "        for witness in witness_list:\n",
    "            # query to see if person already exists in db, if no results found, then add them\n",
    "            person_query = session.query(Person).filter(Person.id == witness['name'])\n",
    "            # if any results are in the list, it will set person_found to True\n",
    "            person_found = False\n",
    "            for person in person_query:\n",
    "                person_found = True\n",
    "            if not person_found:\n",
    "                try:\n",
    "                    session.add(Person(\n",
    "                        id=witness['name'],\n",
    "                        description=witness['description'],\n",
    "                        link=witness['link']\n",
    "                    ))\n",
    "                    print('Added person {}'.format(witness['name']))\n",
    "                    session.commit()\n",
    "                except:\n",
    "                    session.rollback()\n",
    "            # add charter/person relationship information to `charter_witnesses` table created above\n",
    "            try:\n",
    "                session.add(CharterWitness(\n",
    "                    charter_id=charter.id,\n",
    "                    person_id=witness['name'],\n",
    "                    role=witness['role'],\n",
    "                    link=str(witnesses_link)\n",
    "                ))\n",
    "                session.commit()\n",
    "            except:\n",
    "                session.rollback()\n",
    "            print('Added person/charter relationship {} -> {}'.format(witness['name'], charter.id))\n",
    "    charter_counter += 1\n",
    "    \n",
    "# commit all changes to the local db\n",
    "session.close()\n",
    "\n",
    "print('Witnesses successfully scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 total charters scraped.\n",
      "Preview of first charter...\n",
      "Regnante in perpetuum domino nostro Iesu Christo saluatore . mense Aprilio . sub die iiii . kalendas Maias . indictione vii . ego                      Æthelberhtus                   rex filio meo Eadbaldo admonitionem catholice fidei optabilem . Nobis est aptum semper inquirere . qualiter per loca sanctorum pro anime remedio uel stabilitate salutis nostre aliquid de portione terre nostre in subsidiis seruorum dei deuotissimam uoluntatem debeamus offerre . Ideoque tibi                      Sancte Andrea tueque ecclesiae que est constituta in ciuitate Hrofibreui                   ubi preesse uidetur Iustus episcopus . trado aliquantulum telluris mei . Hic est terminus mei doni . Fram suðgeate west andlanges wealles oð norðlanan to stræte . 7 swa east fram st'r'æte oð Doddinghyrnan ongean bradgeat .Siquis uero augere uoluerit hanc ipsam donationem; augeat illi dominus dies bonos . Et si presumpserit minuere aut contradicere; in conspectu dei sit damnatus et sanctorum eius hic et in eterna secula . nisi emendauerit ante eius transitum quod inique gessit contra Christianitatem nostram .Hoc cum consilio Laurentii episcopi et omnium principum meorum signo sancte crucis confirmaui . eosque iussi ut mecum idem facerent . Amen .\n"
     ]
    }
   ],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session(autoflush=False)\n",
    "\n",
    "charters = session.query(Charter)\n",
    "\n",
    "charter_counter = 0\n",
    "for charter in charters:\n",
    "    charter_counter += 1\n",
    "    \n",
    "print('{} total charters scraped.'.format(charter_counter))\n",
    "print('Preview of first charter...')\n",
    "print(charters[0].text.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
