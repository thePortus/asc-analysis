{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and Storing\n",
    "\n",
    "[David J. Thomas](mailto:dave.a.base@gmail.com), [thePortus.com](http://thePortus.com)<br />\n",
    "Instructor of Ancient History and Digital Humanities,<br />\n",
    "Department of History,<br />\n",
    "[University of South Florida](https://github.com/usf-portal)\n",
    "\n",
    "---\n",
    "\n",
    "## This workbook will...\n",
    "\n",
    "* Create a local db to store the data\n",
    "* Scrape/save charter info from ASC and PASE\n",
    "* Scrape/save witness info from PASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Import Module Dependencies\n",
    "\n",
    "The cell below loads all other Python packages needed. You **must** run this before any other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import networkx as nx\n",
    "from IPython.display import clear_output\n",
    "\n",
    "DB_PATH = 'sqlite://'\n",
    "SCRAPE_DELAY = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create Database/Schema\n",
    "\n",
    "Before we try to scrape data from the website, we need to have a place to store it. Rather than export the data as a spreadsheet, storing it as a local database will allow us to perform far more powerful kinds of analysis in later steps. In addition, using the database will allow us to easily export our information in a variety of formats.\n",
    "\n",
    "We are going to use a simple type of database, sqlite. In order to simplify interaction with the database, we are going to use the Python package [SQLAlchemy](https://www.sqlalchemy.org/). This will allow us to easily get related bits of data in a 'Pythonic' way. For example, to get all the people appearing on a charter named `charter` you would write `charter.people`, which will give you a list populated with the relevant items.\n",
    "\n",
    "The code below first defines the database, and also three 'models'... `Charter`, `Person`, and a third table which will store the relational information (which people appeared on which charters). You can envision the way the data for each these models are stored as something like a spreadsheet. The `Person` model is actually stored in a table named `people`, which has 3 columns.\n",
    "\n",
    "After defining these models inside Python using [SQLAlchemy](https://www.sqlalchemy.org/), the last line of code actually commits these changes to the database, which should now have three empty tables, named `charters`, `people`, and `charter_witnesses`. If you want to manually examine the database, you can use a free program like [SQLite Browser](http://sqlitebrowser.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sql.create_engine(DB_PATH, encoding='utf-8')\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Charter(Base):\n",
    "    __tablename__ = 'charters'\n",
    "\n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    sawyer = sql.Column(sql.Integer)\n",
    "    birch = sql.Column(sql.Integer)\n",
    "    kemble = sql.Column(sql.Integer)\n",
    "    british_academy = sql.Column(sql.String)\n",
    "    source_used = sql.Column(sql.String)\n",
    "    archive = sql.Column(sql.String)\n",
    "    language = sql.Column(sql.String)\n",
    "    date = sql.Column(sql.Integer)\n",
    "    scholarly_date = sql.Column(sql.String)\n",
    "    scholarly_date_low = sql.Column(sql.Integer)\n",
    "    scholarly_date_high = sql.Column(sql.Integer)\n",
    "    scholarly_date_avg = sql.Column(sql.Float)\n",
    "    text = sql.Column(sql.Text)\n",
    "    notes = sql.Column(sql.Text)\n",
    "    asc_source = sql.Column(sql.String)\n",
    "    pase_source = sql.Column(sql.String)\n",
    "    pase_witnesses = sql.Column(sql.String)\n",
    "    \n",
    "    witnesses = sql.orm.relationship('Person', secondary='charter_witnesses', back_populates='charters')\n",
    "    \n",
    "    @classmethod\n",
    "    def scrape(cls, url):\n",
    "        # get portions of page for following steps\n",
    "        content = page_soup.body.div.table.find('td', id='content').div\n",
    "        navbar = content.find('ul', class_='charter-nav')\n",
    "        # get various properties from the page\n",
    "        \n",
    "        # get any navigation links\n",
    "        pase_witnesses = navbar.find('li', class_='charter-pase-source').find('a')['href']\n",
    "        # fixes link on asc page, which points to an obsolete address\n",
    "        pase_witnesses = raw_url.replace('ASC', 'Sources').replace('source.jsp', 'DisplaySource.jsp')\n",
    "        \n",
    "    @property\n",
    "    def record(self):\n",
    "        \"\"\"Gets entry data in dictionary format.\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'label': self.id,\n",
    "            'sawyer': self.sawyer,\n",
    "            'birch': self.birch,\n",
    "            'kemble': self.kemble,\n",
    "            'british_academy': self.british_academy,\n",
    "            'source_used': self.source_used,\n",
    "            'archive': self.archive,\n",
    "            'language': self.language,\n",
    "            'date': self.date,\n",
    "            'scholarly_date': self.scholarly_date,\n",
    "            'scholarly_date_low': self.scholarly_date_low,\n",
    "            'scholarly_date_high': self.scholarly_date_high,\n",
    "            'scholarly_date_avg': self.scholarly_date_avg,\n",
    "            'text': self.text,\n",
    "            'notes': self.notes,\n",
    "            'asc_source': self.asc_source,\n",
    "            'pase_source': self.pase_source,\n",
    "            'pase_witnesses': self.pase_witnesses\n",
    "        }\n",
    "    \n",
    "class Person(Base):\n",
    "    __tablename__ = 'people'\n",
    "    \n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    link = sql.Column(sql.String)\n",
    "    \n",
    "    charters = sql.orm.relationship('Charter', secondary='charter_witnesses', back_populates='witnesses')\n",
    "    \n",
    "    @property\n",
    "    def record(self):\n",
    "        \"\"\"Gets entry data in dictionary format.\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'label': self.id + ': ' + self.description,\n",
    "            'link': self.link\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def earliest_appearance(self):\n",
    "        \"\"\"Returns the date of the earliest charter features said person.\"\"\"\n",
    "        earliest_charter = None\n",
    "        for charter in self.charters:\n",
    "            if not earliest_charter:\n",
    "                earliest_charter = charter.scholarly_date_avg\n",
    "            else:\n",
    "                if charter.scholarly_date_avg < earliest_charter:\n",
    "                    earliest_charter = charter.scholarly_date_avg\n",
    "        return earliest_charter\n",
    "\n",
    "    \n",
    "    def num_coappearances(self, other_person):\n",
    "        \"\"\"Checks the number of times this person appears in the same charters as other_person\"\"\"\n",
    "        total_counter = 0\n",
    "        for charter in self.charters:\n",
    "            for other_charter in other_person.charters:\n",
    "                if charter.id == other_charter.id:\n",
    "                    total_counter += 1\n",
    "        return total_counter\n",
    "    \n",
    "    \n",
    "class CharterWitness(Base):\n",
    "    __tablename__ = 'charter_witnesses'\n",
    "    charter_id = sql.Column(sql.String, sql.ForeignKey('charters.id'), primary_key=True) \n",
    "    person_id = sql.Column(sql.String, sql.ForeignKey('people.id'), primary_key=True)\n",
    "    role = sql.Column(sql.String)\n",
    "    link = sql.Column(sql.String)\n",
    "    \n",
    "    @property\n",
    "    def record(self):\n",
    "        \"\"\"Gets entry data in dictionary format.\"\"\"\n",
    "        return {\n",
    "            'charter_id': self.charter_id,\n",
    "            'person_id': self.person_id,\n",
    "            'label': self.role,\n",
    "            'link': self.link\n",
    "        }\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "print('Database Configured Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Scrape ASC and PASE for Charter Info\n",
    "\n",
    "Our first step will be to get the urls for every charter in the ASC database. Then, using `ASCCharterPage` each page of the ASC database will be requested and parsed into a [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) object.\n",
    "\n",
    "As each ASC page is scraped, the link to charter on the PASE database will be used to instantiate a corresponding `PASECharterPage` object. This object will then be used to grab further information about the charter not located on the ASC page. We will also grab the link to people appearing on the charter from each ASC page which we will use in the following step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Make Function to Get HTML and Parse into BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_page(url):\n",
    "    page_html = None\n",
    "    if type(url) != str:\n",
    "        raise Exception('URL must be a valid string')\n",
    "    # enforce a time delay between each scrape\n",
    "    time.sleep(SCRAPE_DELAY)\n",
    "    # attempt to get page data\n",
    "    try:\n",
    "        page_html = requests.get(url).text\n",
    "    # if an error occured, retry by returning recursively\n",
    "    except:\n",
    "        return soup_page(url)\n",
    "    # if all went well, create new BeautifulSoup with page html\n",
    "    return BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "print('Function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Define Object to Scrape Info from a ASC Charter Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASCCharterPage:\n",
    "    \"\"\"Extracts data from a single charter from the ASC database.\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self._url = url\n",
    "        # fetch page and parse into beautifulsoup object\n",
    "        page_soup = soup_page(url)\n",
    "        # get only portion of page with charter-specific content\n",
    "        self.data = page_soup.body.div.table.find('td', id='content').div\n",
    "        # eager load navbar to speed up link retreival\n",
    "        self._navbar = self.data.find('ul', class_='charter-nav')\n",
    "            \n",
    "    @property\n",
    "    def id(self):\n",
    "        \"\"\"Gives the charter ID, with spaces removed.\"\"\"\n",
    "        return self.data.div.div.h1.get_text().replace(' ', '')\n",
    "        \n",
    "    @property\n",
    "    def pase_source(self):\n",
    "        \"\"\"Url to source in PASE database, if extant, otherwise None.\"\"\"\n",
    "        raw_url = self._navbar.find('li', class_='charter-pase-source').find('a')['href']\n",
    "        # fixes link on asc page, which points to an obsolete address\n",
    "        return raw_url.replace('ASC', 'Sources').replace('source.jsp', 'DisplaySource.jsp')\n",
    "        \n",
    "    @property\n",
    "    def pase_witnesses(self):\n",
    "        \"\"\"URL to list of people appearing on charter in the PASE database.\"\"\"\n",
    "        try:\n",
    "            return self._navbar.find('li', class_='charter-pase-witnesses').find('a')['href']\n",
    "        # return None if no witnesses are found (i.e. no link exists)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def description(self):\n",
    "        \"\"\"Modern description of charter.\"\"\"\n",
    "        return self.data.p.get_text()\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        \"\"\"Full text of the charter, in original language. Editorial clause markings removed.\"\"\"\n",
    "        # grab text and convert from latin-1 to utf-8 encoding\n",
    "        raw_text = self.data.find_all('div')[3].get_text()\n",
    "        clean_text = bytearray(raw_text, 'latin-1').decode('utf-8')\n",
    "        # remove text of embedded editorial marks\n",
    "        remove_phrases = [\n",
    "            'DATING CLAUSE', 'INVOCATION', 'PROMULGATION PLACE', 'CURSE',\n",
    "            'DISPOSITIVE WORD', 'BOUNDS', 'PROEM',\n",
    "        ]\n",
    "        for remove_phrase in remove_phrases:\n",
    "            clean_text = clean_text.replace(remove_phrase, '')\n",
    "        # removes extra whitespace by spliting into list of words and rejoining\n",
    "        return clean_text\n",
    "    \n",
    "print('ASC Charter Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Define Object to Scrape Info from a PASE Charter Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASECharterPage:\n",
    "    \"\"\"Extracts further information about charters from their PASE page.\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        # fetch page and convert to beautifulsoup object\n",
    "        self.data = soup_page(url)\n",
    "        sections = self.data.find_all('div', class_='t01')\n",
    "        self._charter_info = sections[0].table.tr.td.table.find_all('tr')\n",
    "        self._source_info = sections[1].table.tr.td.table.find_all('tr')\n",
    "    \n",
    "    @property\n",
    "    def sawyer(self):\n",
    "        \"\"\"Returns the sawyer number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def birch(self):\n",
    "        \"\"\"Returns the birch number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def kemble(self):\n",
    "        \"\"\"Returns the kemble number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[2].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def british_academy(self):\n",
    "        \"\"\"Returns the British Academy reference, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[3].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def source_used(self):\n",
    "        \"\"\"Gives modern source used.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[4].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def archive(self):\n",
    "        \"\"\"Gives name of modern archive housing the charter.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[5].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def language(self):\n",
    "        \"\"\"Gives language(s) used in charter.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def date(self):\n",
    "        \"\"\"Gives long-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def scholarly_date(self):\n",
    "        \"\"\"Gives short-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[2].td.get_text()\n",
    "            \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_low(self):\n",
    "        \"\"\"Will return low date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[0].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_high(self):\n",
    "        \"\"\"Will return high date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[1].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_avg(self):\n",
    "        \"\"\"Returns mean date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        low_date = self.scholarly_date_low\n",
    "        high_date = self.scholarly_date_high\n",
    "        if low_date == high_date:\n",
    "            return low_date\n",
    "        else:\n",
    "            return round((low_date + high_date) / 2, 2)\n",
    "    \n",
    "    @property\n",
    "    def notes(self):\n",
    "        \"\"\"Gives miscellaneous notes on charter.\"\"\"\n",
    "        try:\n",
    "            return self.data.find('div', class_='rec').p.get_text()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "print('PASE Charter Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Scrape Charter Info from ASC and PASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to open a session with the local database\n",
    "session = sessionmaker(bind=engine)()\n",
    "\n",
    "# before scraping information, we need to get the urls for every ASC charter\n",
    "asc_links = []\n",
    "charter_soup = None\n",
    "# fetch page and parse into beautifulsoup object\n",
    "page_soup = soup_page('http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html')\n",
    "# get only portion of page with charter-specific content\n",
    "charter_soup = page_soup.body.div.table.find('tr', class_='r02').find('td', id='content').div\n",
    "# looping through each section and group of rulers\n",
    "for ruler_section in charter_soup.find_all('ul', class_='asc-expand'):\n",
    "    for ruler_group in ruler_section.find_all('li'):\n",
    "        # get relative links from <a> tags append full link to self.data by adding root_url\n",
    "        for charter_link_wrapper in ruler_group.find_all('li'):\n",
    "            asc_links.append('http://www.aschart.kcl.ac.uk' + charter_link_wrapper.a['href'])\n",
    "\n",
    "charter_counter = 0\n",
    "# loop through each charter found online\n",
    "for charter_url in asc_links:\n",
    "    clear_output()\n",
    "    print('Gathering charters: {} of {}...'.format(charter_counter + 1, 467))\n",
    "    try:\n",
    "        asc_charter_page = ASCCharterPage(charter_url)\n",
    "        pase_charter_page = PASECharterPage(asc_charter_page.pase_source)\n",
    "        # make new charter and add to session\n",
    "        session.add(Charter(\n",
    "            id=asc_charter_page.id,\n",
    "            description=asc_charter_page.description,\n",
    "            sawyer=pase_charter_page.sawyer,\n",
    "            birch=pase_charter_page.birch,\n",
    "            kemble=pase_charter_page.kemble,\n",
    "            british_academy=pase_charter_page.british_academy,\n",
    "            source_used=pase_charter_page.source_used,\n",
    "            archive=pase_charter_page.archive,\n",
    "            language=pase_charter_page.language,\n",
    "            date=pase_charter_page.date,\n",
    "            scholarly_date=pase_charter_page.scholarly_date,\n",
    "            scholarly_date_low=pase_charter_page.scholarly_date_low,\n",
    "            scholarly_date_high=pase_charter_page.scholarly_date_high,\n",
    "            scholarly_date_avg=pase_charter_page.scholarly_date_avg,\n",
    "            text=asc_charter_page.text,\n",
    "            notes=pase_charter_page.notes,\n",
    "            asc_source=asc_charter_page._url,\n",
    "            pase_source=asc_charter_page.pase_source,\n",
    "            pase_witnesses=asc_charter_page.pase_witnesses\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print('Error loading page at', charter_url, '(skipped)')\n",
    "    charter_counter += 1\n",
    "# commit all changes to the local db\n",
    "session.commit()\n",
    "session.close()\n",
    "\n",
    "print('Charters successfully scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Scrape PASE for People in Charters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) Define Object to Scrape Info from a PASE Charter Witnesses Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASEWitnesses:\n",
    "    \"\"\"Gets basic information about people on the charter from a PASE witnesses page\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self._url = url\n",
    "        self._soup = soup_page(self._url)\n",
    "        # get only portion of page with relevant data\n",
    "        try:\n",
    "            self.data = self._soup.find('div', class_='rec').find('ul').find_all('li')\n",
    "        # sometimes notes preceed data, in which case get second div with class rec\n",
    "        except:\n",
    "            self.data = self._soup.find_all('div', class_='rec')[1].find('ul').find_all('li')\n",
    "    @property\n",
    "    def witnesses(self):\n",
    "        witness_list = []\n",
    "        for witness_entry in self.data:\n",
    "            witness_link_element = witness_entry.find('a')\n",
    "            try:\n",
    "                witness_role = witness_entry.find('strong').get_text()\n",
    "            except:\n",
    "                witness_role = 'Witness'\n",
    "            witness_list.append({\n",
    "                    'role': witness_role,\n",
    "                    'name': witness_link_element.get_text().lstrip(),\n",
    "                    'link': witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                    'description': witness_entry.find('em').get_text()\n",
    "                })\n",
    "            # look for nested witnesses, sometimes buried in recursive em tags\n",
    "            nested_witnesses_element = None\n",
    "            try:\n",
    "                nested_witnesses_element = self._soup.find('div', class_='rec').find('ul').find('em')\n",
    "            except:\n",
    "                nested_witnesses_element = self._soup.find_all('div', class_='rec')[1].find('ul').find('em')\n",
    "            while nested_witnesses_element is not None:\n",
    "                nested_witnesses = nested_witnesses_element.find_all('li')\n",
    "                for nested_witness_entry in nested_witnesses:\n",
    "                    nested_witness_link_element = witness_entry.find('a')\n",
    "                    try:\n",
    "                        nested_witness_role = nested_witness_entry.find('strong').get_text()\n",
    "                    except:\n",
    "                        nested_witness_role = 'Witness'\n",
    "                    witness_list.append({\n",
    "                        'role': nested_witness_role,\n",
    "                        'name': nested_witness_link_element.get_text(),\n",
    "                        'link': nested_witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                        'description': nested_witness_entry.find('em').get_text()\n",
    "                    })\n",
    "                nested_witnesses_element = nested_witnesses_element.find('em')\n",
    "        return witness_list\n",
    "    \n",
    "\n",
    "print('PASE Witnesses Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) Scrape Witness Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session(autoflush=False)\n",
    "\n",
    "charter_counter = 0\n",
    "# now we want to query our local db for every charter, which is returned as a Charter object\n",
    "for charter in session.query(Charter):\n",
    "    clear_output()\n",
    "    print('Gathering witnesses from charter {} of {}...'.format(charter_counter + 1, 467))\n",
    "    # then get the link to the corresponding PASE page with witness information, skip if there is no link\n",
    "    witnesses_link = charter.pase_witnesses\n",
    "    if witnesses_link is not None:\n",
    "        # use the scraper to get witness data as list of dicts, only proceed if results were found\n",
    "        witness_list = PASEWitnesses(witnesses_link).witnesses\n",
    "        # loop through the list of witnesses\n",
    "        for witness in witness_list:\n",
    "            # query to see if person already exists in db, if no results found, then add them\n",
    "            person_query = session.query(Person).filter(Person.id == witness['name'])\n",
    "            # if any results are in the list, it will set person_found to True\n",
    "            person_found = False\n",
    "            for person in person_query:\n",
    "                person_found = True\n",
    "            if not person_found:\n",
    "                try:\n",
    "                    session.add(Person(\n",
    "                        id=witness['name'],\n",
    "                        description=witness['description'],\n",
    "                        link=witness['link']\n",
    "                    ))\n",
    "                    print('Added person {}'.format(witness['name']))\n",
    "                    session.commit()\n",
    "                except:\n",
    "                    session.rollback()\n",
    "            # add charter/person relationship information to `charter_witnesses` table created above\n",
    "            try:\n",
    "                session.add(CharterWitness(\n",
    "                    charter_id=charter.id,\n",
    "                    person_id=witness['name'],\n",
    "                    role=witness['role'],\n",
    "                    link=str(witnesses_link)\n",
    "                ))\n",
    "                session.commit()\n",
    "            except:\n",
    "                session.rollback()\n",
    "            print('Added person/charter relationship {} -> {}'.format(witness['name'], charter.id))\n",
    "    charter_counter += 1\n",
    "    \n",
    "# commit all changes to the local db\n",
    "session.close()\n",
    "\n",
    "print('Witnesses successfully scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session(autoflush=False)\n",
    "\n",
    "charters = session.query(Charter)\n",
    "\n",
    "charter_counter = 0\n",
    "for charter in charters:\n",
    "    charter_counter += 1\n",
    "    \n",
    "print('{} total charters scraped.'.format(charter_counter))\n",
    "print('Preview of first charter...')\n",
    "print(charters[0].text.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Export Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a) Charters as Plain Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exporting charters as a folder of .txt files')\n",
    "\n",
    "session = sessionmaker(bind=engine)()\n",
    "charters = session.query(Charter)\n",
    "session.close()\n",
    "\n",
    "export_folder_path = os.path.join('../export', 'raw_charter_txts')\n",
    "# ensure parent directory exists, if not, make it\n",
    "if not os.path.exists(export_folder_path):\n",
    "    os.makedirs(export_folder_path)\n",
    "\n",
    "for charter in charters:\n",
    "    output_path = os.path.join(export_folder_path, charter.id + '.txt')\n",
    "    with open(output_path, mode='w+') as txtfile:\n",
    "        txtfile.write(charter.text)\n",
    "        \n",
    "print('Successfully exported to export/raw_charter_txts/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Charters and People as CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exporting charters as a .csv file')\n",
    "\n",
    "session = sessionmaker(bind=engine)()\n",
    "charters = session.query(Charter)\n",
    "session.close()\n",
    "\n",
    "export_path = os.path.join('../export', 'raw_charters.csv')\n",
    "# ensure parent directory exists, if not, make it\n",
    "if not os.path.exists(export_folder_path):\n",
    "    os.makedirs(export_folder_path)\n",
    "\n",
    "fieldnames = [\n",
    "    'id', 'description', 'sawyer', 'birch', 'kemble', 'british_academy', 'source_used', 'archive',\n",
    "    'language', 'date', 'scholarly_date', 'scholarly_date_low', 'scholarly_date_high',\n",
    "    'scholarly_date_avg', 'text', 'notes', 'asc_source', 'pase_source', 'pase_witnesses'\n",
    "]\n",
    "\n",
    "# open the csv file for writing\n",
    "with open(export_path, mode='w+', encoding='utf-8') as csvfile:\n",
    "    # create the csv writer for the file and write the header row\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    # loop through charters\n",
    "    for charter in charters:\n",
    "        new_record = charter.record\n",
    "        # move the 'label' property to the 'description' property\n",
    "        new_record['description'] = new_record['label']\n",
    "        del(new_record['label'])\n",
    "        # write new csv row\n",
    "        writer.writerow(new_record)\n",
    "        \n",
    "print('Successfully exported to export/raw_charters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c) Coappearances of Witnesses as Network File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sessionmaker(bind=engine)()\n",
    "witnesses = session.query(Person)\n",
    "session.close()\n",
    "\n",
    "print('Building network graph...', end='')\n",
    "# create an empty networkx graph\n",
    "witness_network = nx.Graph(\n",
    "    label='',\n",
    "    link=None,\n",
    "    weight=1,\n",
    "    type='Undirected',\n",
    "    community=None,\n",
    "    degree=0,\n",
    "    degree_centrality=0,\n",
    "    betweeness_centrality=0,\n",
    "    eigenvector_centrality=0,\n",
    "    closeness_centrality=0,\n",
    "    harmonic_centrality=0,\n",
    ")\n",
    "\n",
    "# populate the nodes with each witness record\n",
    "for person in witnesses:\n",
    "    witness_network.add_node(person.id, **person.record)\n",
    "\n",
    "counter = 0\n",
    "# to build edges, we need to compare every witness against each other for the number of times they appear\n",
    "for index, person in enumerate(witnesses):\n",
    "    for other_person in witnesses[index + 1:]:\n",
    "        counter += 1\n",
    "        if counter % 50000 == 0:\n",
    "            print('.', end='')\n",
    "        num_coappearances = person.num_coappearances(other_person)\n",
    "        # only add an edge if they actually appeared together\n",
    "        if num_coappearances > 0:\n",
    "            witness_network.add_edge(\n",
    "                person.id,\n",
    "                other_person.id,\n",
    "                label='{} -> {}'.format(person.id, other_person.id),\n",
    "                weight=person.num_coappearances(other_person),\n",
    "                type='Undirected'\n",
    "            )\n",
    "\n",
    "print(' Done!\\n\\nExporting network to `export/witness_network.gexf`...')\n",
    "nx.write_gexf(witness_network, '../export/witness_network.gexf')\n",
    "print('Exported network successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
