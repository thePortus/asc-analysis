{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and Storing\n",
    "\n",
    "[David J. Thomas](mailto:dave.a.base@gmail.com), [thePort.us](http://thePort.us)<br />\n",
    "Instructor of Ancient History and Digital Humanities,<br />\n",
    "Department of History,<br />\n",
    "[University of South Florida](https://github.com/usf-portal)\n",
    "\n",
    "---\n",
    "\n",
    "## This workbook will...\n",
    "\n",
    "* Create a local db to store the data\n",
    "* Scrape/save charter info from ASC and PASE\n",
    "* Scrape/save witness info from PASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Import Module Dependencies\n",
    "\n",
    "The cell below loads all other Python packages needed. You **must** run this before any other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import networkx as nx\n",
    "from IPython.display import clear_output\n",
    "\n",
    "DB_PATH = 'sqlite:///charters.db'\n",
    "SCRAPE_DELAY = 0.5",
    "\n",
    "print('Modules imported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create Database/Schema\n",
    "\n",
    "Before we try to scrape data from the website, we need to have a place to store it. Rather than export the data as a spreadsheet, storing it as a local database will allow us to perform far more powerful kinds of analysis in later steps. In addition, using the database will allow us to easily export our information in a variety of formats.\n",
    "\n",
    "We are going to use a simple type of database, sqlite. In order to simplify interaction with the database, we are going to use the Python package [SQLAlchemy](https://www.sqlalchemy.org/). This will allow us to easily get related bits of data in a 'Pythonic' way. For example, to get all the people appearing on a charter named `charter` you would write `charter.people`, which will give you a list populated with the relevant items.\n",
    "\n",
    "The code below first defines the database, and also three 'models'... `Charter`, `Person`, and a third table which will store the relational information (which people appeared on which charters). You can envision the way the data for each these models are stored as something like a spreadsheet. The `Person` model is actually stored in a table named `people`, which has 3 columns.\n",
    "\n",
    "After defining these models inside Python using [SQLAlchemy](https://www.sqlalchemy.org/), the last line of code actually commits these changes to the database, which should now have three empty tables, named `charters`, `people`, and `charter_witnesses`. If you want to manually examine the database, you can use a free program like [SQLite Browser](http://sqlitebrowser.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sql.create_engine(DB_PATH, encoding='utf-8')\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Charter(Base):\n",
    "    \"\"\"Table for each medieval charter (usually a grant of land from a king to a monastery).\"\"\"\n",
    "    __tablename__ = 'charters'\n",
    "\n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    sawyer = sql.Column(sql.Integer)\n",
    "    birch = sql.Column(sql.Integer)\n",
    "    kemble = sql.Column(sql.Integer)\n",
    "    british_academy = sql.Column(sql.String)\n",
    "    source_used = sql.Column(sql.String)\n",
    "    archive = sql.Column(sql.String)\n",
    "    language = sql.Column(sql.String)\n",
    "    date = sql.Column(sql.Integer)\n",
    "    scholarly_date = sql.Column(sql.String)\n",
    "    scholarly_date_low = sql.Column(sql.Integer)\n",
    "    scholarly_date_high = sql.Column(sql.Integer)\n",
    "    scholarly_date_avg = sql.Column(sql.Float)\n",
    "    text = sql.Column(sql.Text)\n",
    "    notes = sql.Column(sql.Text)\n",
    "    asc_source = sql.Column(sql.String)\n",
    "    pase_source = sql.Column(sql.String)\n",
    "    pase_witnesses = sql.Column(sql.String)\n",
    "    \n",
    "    witnesses = sql.orm.relationship('Person', secondary='charter_witnesses', back_populates='charters')\n",
    "    \n",
    "    @classmethod\n",
    "    def scrape(cls, url):\n",
    "        # get portions of page for following steps\n",
    "        content = page_soup.body.div.table.find('td', id='content').div\n",
    "        navbar = content.find('ul', class_='charter-nav')\n",
    "        # get various properties from the page\n",
    "        \n",
    "        # get any navigation links\n",
    "        pase_witnesses = navbar.find('li', class_='charter-pase-source').find('a')['href']\n",
    "        # fixes link on asc page, which points to an obsolete address\n",
    "        pase_witnesses = raw_url.replace('ASC', 'Sources').replace('source.jsp', 'DisplaySource.jsp')\n",
    "        \n",
    "    @property\n",
    "    def record(self):\n",
    "        \"\"\"Gets entry data in dictionary format.\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'label': self.id,\n",
    "            'sawyer': self.sawyer,\n",
    "            'birch': self.birch,\n",
    "            'kemble': self.kemble,\n",
    "            'british_academy': self.british_academy,\n",
    "            'source_used': self.source_used,\n",
    "            'archive': self.archive,\n",
    "            'language': self.language,\n",
    "            'date': self.date,\n",
    "            'scholarly_date': self.scholarly_date,\n",
    "            'scholarly_date_low': self.scholarly_date_low,\n",
    "            'scholarly_date_high': self.scholarly_date_high,\n",
    "            'scholarly_date_avg': self.scholarly_date_avg,\n",
    "            'text': self.text,\n",
    "            'notes': self.notes,\n",
    "            'asc_source': self.asc_source,\n",
    "            'pase_source': self.pase_source,\n",
    "            'pase_witnesses': self.pase_witnesses\n",
    "        }\n",
    "    \n",
    "class Person(Base):\n",
    "    \"\"\"Table for each person, usually a witness who appeared in a charter.\"\"\"\n",
    "    __tablename__ = 'people'\n",
    "    \n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    link = sql.Column(sql.String)\n",
    "    \n",
    "    charters = sql.orm.relationship('Charter', secondary='charter_witnesses', back_populates='witnesses')\n",
    "    \n",
    "    @property\n",
    "    def record(self):\n",
    "        \"\"\"Gets entry data in dictionary format.\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'label': self.id + ': ' + self.description,\n",
    "            'link': self.link\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def earliest_appearance(self):\n",
    "        \"\"\"Returns the date of the earliest charter features said person.\"\"\"\n",
    "        earliest_charter = None\n",
    "        for charter in self.charters:\n",
    "            if not earliest_charter:\n",
    "                earliest_charter = charter.scholarly_date_avg\n",
    "            else:\n",
    "                if charter.scholarly_date_avg < earliest_charter:\n",
    "                    earliest_charter = charter.scholarly_date_avg\n",
    "        return earliest_charter\n",
    "\n",
    "    \n",
    "    def num_coappearances(self, other_person):\n",
    "        \"\"\"Checks the number of times this person appears in the same charters as other_person\"\"\"\n",
    "        total_counter = 0\n",
    "        for charter in self.charters:\n",
    "            for other_charter in other_person.charters:\n",
    "                if charter.id == other_charter.id:\n",
    "                    total_counter += 1\n",
    "        return total_counter\n",
    "    \n",
    "    \n",
    "class CharterWitness(Base):\n",
    "    \"\"\"Relational n2n table connecting people to the charters in which they were witnesses\"\"\"\n",
    "    __tablename__ = 'charter_witnesses'\n",
    "    charter_id = sql.Column(sql.String, sql.ForeignKey('charters.id'), primary_key=True) \n",
    "    person_id = sql.Column(sql.String, sql.ForeignKey('people.id'), primary_key=True)\n",
    "    role = sql.Column(sql.String)\n",
    "    link = sql.Column(sql.String)\n",
    "    \n",
    "    @property\n",
    "    def record(self):\n",
    "        \"\"\"Gets entry data in dictionary format.\"\"\"\n",
    "        return {\n",
    "            'charter_id': self.charter_id,\n",
    "            'person_id': self.person_id,\n",
    "            'label': self.role,\n",
    "            'link': self.link\n",
    "        }\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "print('Database Configured Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Scrape ASC and PASE for Charter Info\n",
    "\n",
    "Our first step will be to get the urls for every charter in the ASC database. Then, using `ASCCharterPage` each page of the ASC database will be requested and parsed into a [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) object.\n",
    "\n",
    "As each ASC page is scraped, the link to charter on the PASE database will be used to instantiate a corresponding `PASECharterPage` object. This object will then be used to grab further information about the charter not located on the ASC page. We will also grab the link to people appearing on the charter from each ASC page which we will use in the following step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Make Function to Get HTML and Parse into BeautifulSoup\n",
    "\n",
    "In order to speed up other objects and avoid repeating code, we are going to create a function that will do all the work of getting a web page and converting it into a BeautifulSoup object, making it easy to scrape for information in a Pythonic way. The function gets a URL, if there is an error it calls itself recursively until a retry limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined.\n"
     ]
    }
   ],
   "source": [
    "def soup_page(url, tries_left = 5):\n",
    "    \"\"\"Receives URL, requests raw html, then returns converted BeautifulSoup object.\"\"\"\n",
    "    page_html = None\n",
    "    if not tries_left or type(tries_left) != int or tries_left < 0:\n",
    "        tries_left = 5\n",
    "    if type(url) != str:\n",
    "        raise Exception('URL must be a valid string')\n",
    "    # enforce a time delay between each scrape\n",
    "    time.sleep(SCRAPE_DELAY)\n",
    "    print('Getting', url)\n",
    "    # attempt to get page data\n",
    "    try:\n",
    "        page_html = requests.get(url).text\n",
    "        tries_left -= 1\n",
    "    # if an error occured, retry by returning recursively\n",
    "    except:\n",
    "        print('Error getting', url)\n",
    "        if tries_left > 0:\n",
    "            print('Retrying...')\n",
    "            return soup_page(url, tries_left=triest_left-1)\n",
    "        if tries_left <= 0:\n",
    "            print('Retry limit reached, ABORTING parse of', url)\n",
    "            return None\n",
    "    print('Success, souping...')\n",
    "    # if all went well, create new BeautifulSoup with page html\n",
    "    return BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "print('Function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Define Object to Scrape Info from a ASC Charter Page\n",
    "\n",
    "Now that we have a function which can give us a parser for any page, we can create an object that will parse and quickly give the data for any *one* ASC charter page. We will use this definition to create a bunch of instances to extract the data from the ASC pages. First through, we need to define the object, and which parts of the page should be tied to which properties and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASC Charter Scraper Defined\n"
     ]
    }
   ],
   "source": [
    "class ASCCharterPage:\n",
    "    \"\"\"Extracts data from a single charter from the ASC database.\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self._url = url\n",
    "        # fetch page and parse into beautifulsoup object\n",
    "        page_soup = soup_page(url)\n",
    "        # get only portion of page with charter-specific content\n",
    "        self.data = page_soup.body.div.table.find('td', id='content').div\n",
    "        # eager load navbar to speed up link retreival\n",
    "        self._navbar = self.data.find('ul', class_='charter-nav')\n",
    "            \n",
    "    @property\n",
    "    def id(self):\n",
    "        \"\"\"Gives the charter ID, with spaces removed.\"\"\"\n",
    "        return self.data.div.div.h1.get_text().replace(' ', '')\n",
    "        \n",
    "    @property\n",
    "    def pase_source(self):\n",
    "        \"\"\"Url to source in PASE database, if extant, otherwise None.\"\"\"\n",
    "        raw_url = self._navbar.find('li', class_='charter-pase-source').find('a')['href']\n",
    "        # fixes link on asc page, which points to an obsolete address\n",
    "        return raw_url.replace('ASC', 'Sources').replace('source.jsp', 'DisplaySource.jsp')\n",
    "        \n",
    "    @property\n",
    "    def pase_witnesses(self):\n",
    "        \"\"\"URL to list of people appearing on charter in the PASE database.\"\"\"\n",
    "        try:\n",
    "            return self._navbar.find('li', class_='charter-pase-witnesses').find('a')['href']\n",
    "        # return None if no witnesses are found (i.e. no link exists)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def description(self):\n",
    "        \"\"\"Modern description of charter.\"\"\"\n",
    "        return self.data.p.get_text()\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        \"\"\"Full text of the charter, in original language. Editorial clause markings removed.\"\"\"\n",
    "        # grab text and convert from latin-1 to utf-8 encoding\n",
    "        raw_text = self.data.find_all('div')[3].get_text()\n",
    "        clean_text = bytearray(raw_text, 'latin-1').decode('utf-8')\n",
    "        # remove text of embedded editorial marks\n",
    "        remove_phrases = [\n",
    "            'DATING CLAUSE', 'INVOCATION', 'PROMULGATION PLACE', 'CURSE',\n",
    "            'DISPOSITIVE WORD', 'BOUNDS', 'PROEM',\n",
    "        ]\n",
    "        for remove_phrase in remove_phrases:\n",
    "            clean_text = clean_text.replace(remove_phrase, '')\n",
    "        # removes extra whitespace by spliting into list of words and rejoining\n",
    "        return clean_text\n",
    "    \n",
    "print('ASC Charter Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Define Object to Scrape Info from a PASE Charter Page\n",
    "\n",
    "Just as we did above for individual ASC charter pages, we need a different object which can rapidly scrape information from each PASE database page related to a charter. By scraping different information from each database about the same charter, we are able to round out our picture of not only the charter but the people on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASE Charter Scraper Defined\n"
     ]
    }
   ],
   "source": [
    "class PASECharterPage:\n",
    "    \"\"\"Extracts further information about charters from their PASE page.\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        # fetch page and convert to beautifulsoup object\n",
    "        self.data = soup_page(url)\n",
    "        sections = self.data.find_all('div', class_='t01')\n",
    "        self._charter_info = sections[0].table.tr.td.table.find_all('tr')\n",
    "        self._source_info = sections[1].table.tr.td.table.find_all('tr')\n",
    "    \n",
    "    @property\n",
    "    def sawyer(self):\n",
    "        \"\"\"Returns the sawyer number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def birch(self):\n",
    "        \"\"\"Returns the birch number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def kemble(self):\n",
    "        \"\"\"Returns the kemble number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[2].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def british_academy(self):\n",
    "        \"\"\"Returns the British Academy reference, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[3].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def source_used(self):\n",
    "        \"\"\"Gives modern source used.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[4].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def archive(self):\n",
    "        \"\"\"Gives name of modern archive housing the charter.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[5].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def language(self):\n",
    "        \"\"\"Gives language(s) used in charter.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def date(self):\n",
    "        \"\"\"Gives long-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def scholarly_date(self):\n",
    "        \"\"\"Gives short-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[2].td.get_text()\n",
    "            \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_low(self):\n",
    "        \"\"\"Will return low date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[0].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_high(self):\n",
    "        \"\"\"Will return high date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[1].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_avg(self):\n",
    "        \"\"\"Returns mean date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        low_date = self.scholarly_date_low\n",
    "        high_date = self.scholarly_date_high\n",
    "        if low_date == high_date:\n",
    "            return low_date\n",
    "        else:\n",
    "            return round((low_date + high_date) / 2, 2)\n",
    "    \n",
    "    @property\n",
    "    def notes(self):\n",
    "        \"\"\"Gives miscellaneous notes on charter.\"\"\"\n",
    "        try:\n",
    "            return self.data.find('div', class_='rec').p.get_text()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "print('PASE Charter Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Scrape Charter Info from ASC and PASE\n",
    "\n",
    "Now that we have both scrapers defined, we can put them to use! Time to download the information and store it in our database. We will use the two scraper objects we created in steps 3a & 3b to get information off of the website and then store it our local database using the models we defined in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering charters: 5 of 467...\n",
      "Getting http://www.aschart.kcl.ac.uk/charters/s0005.html\n",
      "Success, souping...\n",
      "Getting http://www.pase.ac.uk/jsp/Sources/DisplaySource.jsp?sourceKey=110\n",
      "Error getting http://www.pase.ac.uk/jsp/Sources/DisplaySource.jsp?sourceKey=110 retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse(buffering=True)\n",
      "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-399e7d1d6f63>\", line 10, in soup_page\n",
      "    page_html = requests.get(url).text\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/requests/api.py\", line 72, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/requests/api.py\", line 58, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/requests/sessions.py\", line 508, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/requests/sessions.py\", line 618, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/requests/adapters.py\", line 440, in send\n",
      "    timeout=timeout\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 601, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 383, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/anaconda3/lib/python3.6/http/client.py\", line 1331, in getresponse\n",
      "    response.begin()\n",
      "  File \"/anaconda3/lib/python3.6/http/client.py\", line 297, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/anaconda3/lib/python3.6/http/client.py\", line 258, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/anaconda3/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-a6c891b23f21>\", line 25, in <module>\n",
      "    pase_charter_page = PASECharterPage(asc_charter_page.pase_source)\n",
      "  File \"<ipython-input-5-5d8f1763e685>\", line 6, in __init__\n",
      "    self.data = soup_page(url)\n",
      "  File \"<ipython-input-3-399e7d1d6f63>\", line 14, in soup_page\n",
      "    return soup_page(url)\n",
      "  File \"<ipython-input-3-399e7d1d6f63>\", line 6, in soup_page\n",
      "    time.sleep(SCRAPE_DELAY)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/anaconda3/lib/python3.6/inspect.py\", line 732, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-399e7d1d6f63>\u001b[0m in \u001b[0;36msoup_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpage_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# if an error occured, retry by returning recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2909\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2910\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2911\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a6c891b23f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0masc_charter_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mASCCharterPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharter_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mpase_charter_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPASECharterPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masc_charter_page\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpase_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# make new charter and add to session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5d8f1763e685>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# fetch page and convert to beautifulsoup object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0msections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m't01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-399e7d1d6f63>\u001b[0m in \u001b[0;36msoup_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error getting'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retrying...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msoup_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Success, souping...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-399e7d1d6f63>\u001b[0m in \u001b[0;36msoup_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# enforce a time delay between each scrape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCRAPE_DELAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Getting'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1827\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1829\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1831\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1371\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1279\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m             )\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mformatted_exceptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not list"
     ]
    }
   ],
   "source": [
    "# first, we need to open a session with the local database\n",
    "session = sessionmaker(bind=engine)()\n",
    "\n",
    "# before scraping information, we need to get the urls for every ASC charter\n",
    "asc_links = []\n",
    "charter_soup = None\n",
    "# fetch page and parse into beautifulsoup object\n",
    "page_soup = soup_page('http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html')\n",
    "# get only portion of page with charter-specific content\n",
    "charter_soup = page_soup.body.div.table.find('tr', class_='r02').find('td', id='content').div\n",
    "# looping through each section and group of rulers\n",
    "for ruler_section in charter_soup.find_all('ul', class_='asc-expand'):\n",
    "    for ruler_group in ruler_section.find_all('li'):\n",
    "        # get relative links from <a> tags append full link to self.data by adding root_url\n",
    "        for charter_link_wrapper in ruler_group.find_all('li'):\n",
    "            # when joining, trim initial two charcters (which are unwanted dots '.')\n",
    "            asc_links.append('http://www.aschart.kcl.ac.uk' + charter_link_wrapper.a['href'][2:])\n",
    "\n",
    "charter_counter = 0\n",
    "# loop through each charter found online\n",
    "for charter_url in asc_links:\n",
    "    clear_output()\n",
    "    print('Gathering charters: {} of {}...'.format(charter_counter + 1, 467))\n",
    "    try:\n",
    "        asc_charter_page = ASCCharterPage(charter_url)\n",
    "        pase_charter_page = PASECharterPage(asc_charter_page.pase_source)\n",
    "        # make new charter and add to session\n",
    "        session.add(Charter(\n",
    "            id=asc_charter_page.id,\n",
    "            description=asc_charter_page.description,\n",
    "            sawyer=pase_charter_page.sawyer,\n",
    "            birch=pase_charter_page.birch,\n",
    "            kemble=pase_charter_page.kemble,\n",
    "            british_academy=pase_charter_page.british_academy,\n",
    "            source_used=pase_charter_page.source_used,\n",
    "            archive=pase_charter_page.archive,\n",
    "            language=pase_charter_page.language,\n",
    "            date=pase_charter_page.date,\n",
    "            scholarly_date=pase_charter_page.scholarly_date,\n",
    "            scholarly_date_low=pase_charter_page.scholarly_date_low,\n",
    "            scholarly_date_high=pase_charter_page.scholarly_date_high,\n",
    "            scholarly_date_avg=pase_charter_page.scholarly_date_avg,\n",
    "            text=asc_charter_page.text,\n",
    "            notes=pase_charter_page.notes,\n",
    "            asc_source=asc_charter_page._url,\n",
    "            pase_source=asc_charter_page.pase_source,\n",
    "            pase_witnesses=asc_charter_page.pase_witnesses\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print('Error loading page at', charter_url, '(skipped)')\n",
    "    charter_counter += 1\n",
    "# commit all changes to the local db\n",
    "session.commit()\n",
    "session.close()\n",
    "\n",
    "print('Charters successfully scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Scrape PASE for People in Charters\n",
    "\n",
    "Great, so we have lots of information about the charters, but knowing something about the social context would add a lot to this. So, time to define yet a third scraper. This one is going to go through the PASE database, but this time it is going to seek information about which people appeared in which medieval charters. This particular exercise doesn't scrape much metadata about the people. Nevertheless, each person has a unique ID number. This is going to allow us to connect people together in a network based upon their shared appearances at witnesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) Define Object to Scrape Info from a PASE Charter Witnesses Page\n",
    "\n",
    "As before, we need to define an object to scrape the page. This one represents a single page from the PASE database containing the list of witnesses that appear on a single charter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASEWitnesses:\n",
    "    \"\"\"Gets basic information about people on the charter from a PASE witnesses page\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self._url = url\n",
    "        self._soup = soup_page(self._url)\n",
    "        # get only portion of page with relevant data\n",
    "        try:\n",
    "            self.data = self._soup.find('div', class_='rec').find('ul').find_all('li')\n",
    "        # sometimes notes preceed data, in which case get second div with class rec\n",
    "        except:\n",
    "            self.data = self._soup.find_all('div', class_='rec')[1].find('ul').find_all('li')\n",
    "    @property\n",
    "    def witnesses(self):\n",
    "        witness_list = []\n",
    "        for witness_entry in self.data:\n",
    "            witness_link_element = witness_entry.find('a')\n",
    "            try:\n",
    "                witness_role = witness_entry.find('strong').get_text()\n",
    "            except:\n",
    "                witness_role = 'Witness'\n",
    "            witness_list.append({\n",
    "                    'role': witness_role,\n",
    "                    'name': witness_link_element.get_text().lstrip(),\n",
    "                    'link': witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                    'description': witness_entry.find('em').get_text()\n",
    "                })\n",
    "            # look for nested witnesses, sometimes buried in recursive em tags\n",
    "            nested_witnesses_element = None\n",
    "            try:\n",
    "                nested_witnesses_element = self._soup.find('div', class_='rec').find('ul').find('em')\n",
    "            except:\n",
    "                nested_witnesses_element = self._soup.find_all('div', class_='rec')[1].find('ul').find('em')\n",
    "            while nested_witnesses_element is not None:\n",
    "                nested_witnesses = nested_witnesses_element.find_all('li')\n",
    "                for nested_witness_entry in nested_witnesses:\n",
    "                    nested_witness_link_element = witness_entry.find('a')\n",
    "                    try:\n",
    "                        nested_witness_role = nested_witness_entry.find('strong').get_text()\n",
    "                    except:\n",
    "                        nested_witness_role = 'Witness'\n",
    "                    witness_list.append({\n",
    "                        'role': nested_witness_role,\n",
    "                        'name': nested_witness_link_element.get_text(),\n",
    "                        'link': nested_witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                        'description': nested_witness_entry.find('em').get_text()\n",
    "                    })\n",
    "                nested_witnesses_element = nested_witnesses_element.find('em')\n",
    "        return witness_list\n",
    "    \n",
    "\n",
    "print('PASE Witnesses Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) Scrape Witness Info\n",
    "\n",
    "As before, we will instantiate our scraper, creating one for each charter. Traversing the list, we will store the information in the Witness and CharterWitness tables. This will allow us to get network information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session(autoflush=False)\n",
    "\n",
    "charter_counter = 0\n",
    "# now we want to query our local db for every charter, which is returned as a Charter object\n",
    "for charter in session.query(Charter):\n",
    "    clear_output()\n",
    "    print('Gathering witnesses from charter {} of {}...'.format(charter_counter + 1, 467))\n",
    "    # then get the link to the corresponding PASE page with witness information, skip if there is no link\n",
    "    witnesses_link = charter.pase_witnesses\n",
    "    if witnesses_link is not None:\n",
    "        # use the scraper to get witness data as list of dicts, only proceed if results were found\n",
    "        witness_list = PASEWitnesses(witnesses_link).witnesses\n",
    "        # loop through the list of witnesses\n",
    "        for witness in witness_list:\n",
    "            # query to see if person already exists in db, if no results found, then add them\n",
    "            person_query = session.query(Person).filter(Person.id == witness['name'])\n",
    "            # if any results are in the list, it will set person_found to True\n",
    "            person_found = False\n",
    "            for person in person_query:\n",
    "                person_found = True\n",
    "            if not person_found:\n",
    "                try:\n",
    "                    session.add(Person(\n",
    "                        id=witness['name'],\n",
    "                        description=witness['description'],\n",
    "                        link=witness['link']\n",
    "                    ))\n",
    "                    print('Added person {}'.format(witness['name']))\n",
    "                    session.commit()\n",
    "                except:\n",
    "                    session.rollback()\n",
    "            # add charter/person relationship information to `charter_witnesses` table created above\n",
    "            try:\n",
    "                session.add(CharterWitness(\n",
    "                    charter_id=charter.id,\n",
    "                    person_id=witness['name'],\n",
    "                    role=witness['role'],\n",
    "                    link=str(witnesses_link)\n",
    "                ))\n",
    "                session.commit()\n",
    "            except:\n",
    "                session.rollback()\n",
    "            print('Added person/charter relationship {} -> {}'.format(witness['name'], charter.id))\n",
    "    charter_counter += 1\n",
    "    \n",
    "# commit all changes to the local db\n",
    "session.close()\n",
    "\n",
    "print('Witnesses successfully scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Preview Results\n",
    "\n",
    "Let's see if it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 total charters scraped.\n",
      "Preview of first charter...\n",
      "Regnante in perpetuum domino nostro Iesu Christo saluatore . mense Aprilio . sub die iiii . kalendas Maias . indictione vii . ego thelberhtus rex filio meo Eadbaldo admonitionem catholice fidei optabilem . Nobis est aptum semper inquirere . qualiter per loca sanctorum pro anime remedio uel stabilitate salutis nostre aliquid de portione terre nostre in subsidiis seruorum dei deuotissimam uoluntatem debeamus offerre . Ideoque tibi Sancte Andrea tueque ecclesiae que est constituta in ciuitate Hrofibreui ubi preesse uidetur Iustus episcopus . trado aliquantulum telluris mei . Hic est terminus mei doni . Fram sugeate west andlanges wealles o norlanan to strte . 7 swa east fram st'r'te o Doddinghyrnan ongean bradgeat . Siquis uero augere uoluerit hanc ipsam donationem; augeat illi dominus dies bonos . Et si presumpserit minuere aut contradicere; in conspectu dei sit damnatus et sanctorum eius hic et in eterna secula . nisi emendauerit ante eius transitum quod inique gessit contra Christianitatem nostram . Hoc cum consilio Laurentii episcopi et omnium principum meorum signo sancte crucis confirmaui . eosque iussi ut mecum idem facerent . Amen .\n"
     ]
    }
   ],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session(autoflush=False)\n",
    "\n",
    "charters = session.query(Charter)\n",
    "\n",
    "charter_counter = 0\n",
    "for charter in charters:\n",
    "    charter_counter += 1\n",
    "    \n",
    "print('{} total charters scraped.'.format(charter_counter))\n",
    "print('Preview of first charter...')\n",
    "print(charters[0].text.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Export Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a) Charters as Plain Text Files\n",
    "\n",
    "Some text analysis programs like to use plain .txt files. This cell will generate a folder full of .txt files, one for each charter. Works well with [Voyant](https://voyant-tools.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting charters as a folder of .txt files\n",
      "Successfully exported to data/raw_charter_txts/\n"
     ]
    }
   ],
   "source": [
    "print('Exporting charters as a folder of .txt files')\n",
    "\n",
    "session = sessionmaker(bind=engine)()\n",
    "charters = session.query(Charter)\n",
    "session.close()\n",
    "\n",
    "export_folder_path = os.path.join('../data', 'raw_charter_txts')\n",
    "# ensure parent directory exists, if not, make it\n",
    "if not os.path.exists(export_folder_path):\n",
    "    os.makedirs(export_folder_path)\n",
    "\n",
    "for charter in charters:\n",
    "    output_path = os.path.join(export_folder_path, charter.id + '.txt')\n",
    "    with open(output_path, mode='w+') as txtfile:\n",
    "        txtfile.write(charter.text)\n",
    "        \n",
    "print('Successfully exported to data/raw_charter_txts/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Charters as CSV Files\n",
    "\n",
    "A more standard format are CSV files. This cell generates one .csv file, with a column for every metadata field. This works well for the majority of text analysis programs and the ability to connect metadata expands the possible levels of analysis. Works well with programs like [Overview](https://www.overviewdocs.com/) or [Orange3](https://orange.biolab.si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting charters as a .csv file\n",
      "Successfully exported to data/raw_charters.csv\n"
     ]
    }
   ],
   "source": [
    "print('Exporting charters as a .csv file')\n",
    "\n",
    "session = sessionmaker(bind=engine)()\n",
    "charters = session.query(Charter)\n",
    "session.close()\n",
    "\n",
    "export_path = os.path.join('../data', 'raw_charters.csv')\n",
    "# ensure parent directory exists, if not, make it\n",
    "if not os.path.exists(export_folder_path):\n",
    "    os.makedirs(export_folder_path)\n",
    "\n",
    "fieldnames = [\n",
    "    'id', 'description', 'sawyer', 'birch', 'kemble', 'british_academy', 'source_used', 'archive',\n",
    "    'language', 'date', 'scholarly_date', 'scholarly_date_low', 'scholarly_date_high',\n",
    "    'scholarly_date_avg', 'text', 'notes', 'asc_source', 'pase_source', 'pase_witnesses'\n",
    "]\n",
    "\n",
    "# open the csv file for writing\n",
    "with open(export_path, mode='w+', encoding='utf-8') as csvfile:\n",
    "    # create the csv writer for the file and write the header row\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    # loop through charters\n",
    "    for charter in charters:\n",
    "        new_record = charter.record\n",
    "        # move the 'label' property to the 'description' property\n",
    "        new_record['description'] = new_record['label']\n",
    "        del(new_record['label'])\n",
    "        # write new csv row\n",
    "        writer.writerow(new_record)\n",
    "        \n",
    "print('Successfully exported to data/raw_charters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c) Coappearances of Witnesses as Network File\n",
    "\n",
    "Now that we have information about charters, and witnesses, and their relationships, we can use the networkx package to easily create a .gexf file, the most commonly used network graph format. It works very well with [Gephi](https://gephi.org/). \n",
    "\n",
    "**This step is CRITICAL if you want to do the next workbook!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network graph........................................ Done!\n",
      "\n",
      "Exporting network to `data/witness_network.gexf`...\n",
      "Exported network successfully!\n"
     ]
    }
   ],
   "source": [
    "session = sessionmaker(bind=engine)()\n",
    "witnesses = session.query(Person)\n",
    "session.close()\n",
    "\n",
    "print('Building network graph...', end='')\n",
    "# create an empty networkx graph\n",
    "witness_network = nx.Graph(\n",
    "    label='',\n",
    "    link=None,\n",
    "    weight=1,\n",
    "    type='Undirected',\n",
    "    community=None,\n",
    "    degree=0,\n",
    "    degree_centrality=0,\n",
    "    betweeness_centrality=0,\n",
    "    eigenvector_centrality=0,\n",
    "    closeness_centrality=0,\n",
    "    harmonic_centrality=0,\n",
    ")\n",
    "\n",
    "# populate the nodes with each witness record\n",
    "for person in witnesses:\n",
    "    witness_network.add_node(person.id, **person.record)\n",
    "\n",
    "counter = 0\n",
    "# to build edges, we need to compare every witness against each other for the number of times they appear\n",
    "for index, person in enumerate(witnesses):\n",
    "    for other_person in witnesses[index + 1:]:\n",
    "        counter += 1\n",
    "        if counter % 50000 == 0:\n",
    "            print('.', end='')\n",
    "        num_coappearances = person.num_coappearances(other_person)\n",
    "        # only add an edge if they actually appeared together\n",
    "        if num_coappearances > 0:\n",
    "            witness_network.add_edge(\n",
    "                person.id,\n",
    "                other_person.id,\n",
    "                label='{} -> {}'.format(person.id, other_person.id),\n",
    "                weight=person.num_coappearances(other_person),\n",
    "                type='Undirected'\n",
    "            )\n",
    "\n",
    "print(' Done!\\n\\nExporting network to `data/witness_network.gexf`...')\n",
    "nx.write_gexf(witness_network, '../data/witness_network.gexf')\n",
    "print('Exported network successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Proceed to the next workbook!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
