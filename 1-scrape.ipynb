{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and Storing\n",
    "\n",
    "[David J. Thomas](mailto:dave.a.base@gmail.com), [thePortus.com](http://thePortus.com)<br />\n",
    "Instructor of Ancient History and Digital Humanities,<br />\n",
    "Department of History,<br />\n",
    "[University of South Florida](https://github.com/usf-portal)\n",
    "\n",
    "---\n",
    "\n",
    "## This workbook will...\n",
    "\n",
    "* Create a local db to store the data\n",
    "* Scrape/save charter info from ASC and PASE\n",
    "* Scrape/save witness info from PASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Import Module Dependencies\n",
    "\n",
    "The cell below loads all other Python packages needed. You **must** run this before any other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from IPython.display import clear_output\n",
    "import dhelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create Database/Schema\n",
    "\n",
    "Before we try to scrape data from the website, we need to have a place to store it. Rather than export the data as a spreadsheet, storing it as a local database will allow us to perform far more powerful kinds of analysis in later steps. In addition, using the database will allow us to easily export our information in a variety of formats.\n",
    "\n",
    "We are going to use a simple type of database, sqlite. In order to simplify interaction with the database, we are going to use the Python package [SQLAlchemy](https://www.sqlalchemy.org/). This will allow us to easily get related bits of data in a 'Pythonic' way. For example, to get all the people appearing on a charter named `charter` you would write `charter.people`, which will give you a list populated with the relevant items.\n",
    "\n",
    "The code below first defines the database, and also three 'models'... `Charter`, `Person`, and a third table which will store the relational information (which people appeared on which charters). You can envision the way the data for each these models are stored as something like a spreadsheet. The `Person` model is actually stored in a table named `people`, which has 3 columns.\n",
    "\n",
    "After defining these models inside Python using [SQLAlchemy](https://www.sqlalchemy.org/), the last line of code actually commits these changes to the database, which should now have three empty tables, named `charters`, `people`, and `charter_witnesses`. If you want to manually examine the database, you can use a free program like [SQLite Browser](http://sqlitebrowser.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Configured Successfully\n"
     ]
    }
   ],
   "source": [
    "engine = sql.create_engine('sqlite:///charters.db', echo=False)\n",
    "Base = declarative_base()\n",
    "    \n",
    "\n",
    "class Charter(Base):\n",
    "    __tablename__ = 'charters'\n",
    "\n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    sawyer = sql.Column(sql.Integer)\n",
    "    birch = sql.Column(sql.Integer)\n",
    "    kemble = sql.Column(sql.Integer)\n",
    "    british_academy = sql.Column(sql.String)\n",
    "    source_used = sql.Column(sql.String)\n",
    "    archive = sql.Column(sql.String)\n",
    "    language = sql.Column(sql.String)\n",
    "    date = sql.Column(sql.Integer)\n",
    "    scholarly_date = sql.Column(sql.String)\n",
    "    scholarly_date_low = sql.Column(sql.Integer)\n",
    "    scholarly_date_high = sql.Column(sql.Integer)\n",
    "    scholarly_date_avg = sql.Column(sql.Float)\n",
    "    text = sql.Column(sql.Text)\n",
    "    notes = sql.Column(sql.Text)\n",
    "    asc_source = sql.Column(sql.String)\n",
    "    pase_source = sql.Column(sql.String)\n",
    "    pase_witnesses = sql.Column(sql.String)\n",
    "    \n",
    "    witnesses = sql.orm.relationship('Person', secondary='charter_witnesses', back_populates='charters')\n",
    "    \n",
    "class Person(Base):\n",
    "    __tablename__ = 'people'\n",
    "    \n",
    "    id = sql.Column(sql.String, primary_key=True)\n",
    "    description = sql.Column(sql.String)\n",
    "    link = sql.Column(sql.String)\n",
    "    \n",
    "    charters = sql.orm.relationship('Charter', secondary='charter_witnesses', back_populates='witnesses')\n",
    "    \n",
    "    @property\n",
    "    def earliest_appearance(self):\n",
    "        \"\"\"Returns the date of the earliest charter features said person.\"\"\"\n",
    "        earliest_charter = None\n",
    "        for charter in self.charters:\n",
    "            if not earliest_charter:\n",
    "                earliest_charter = charter.scholarly_date_avg\n",
    "            else:\n",
    "                if charter.scholarly_date_avg < earliest_charter:\n",
    "                    earliest_charter = charter.scholarly_date_avg\n",
    "        return earliest_charter\n",
    "    \n",
    "    \n",
    "class CharterWitness(Base):\n",
    "    __tablename__ = 'charter_witnesses'\n",
    "    charter_id = sql.Column(sql.String, sql.ForeignKey('charters.id'), primary_key=True) \n",
    "    person_id = sql.Column(sql.String, sql.ForeignKey('people.id'), primary_key=True)\n",
    "    role = sql.Column(sql.String)\n",
    "    link = sql.Column(sql.String)\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "print('Database Configured Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Scrape ASC and PASE for Charter Info\n",
    "\n",
    "Our first step will be to get the urls for every charter in the ASC database. Then, using `ASCCharterPage` each page of the ASC database will be requested and parsed into a [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) object using the [dhelp](https://github.com/thePortus/dhelp) package.\n",
    "\n",
    "As each ASC page is scraped, the link to charter on the PASE database will be used to instantiate a corresponding `PASECharterPage` object. This object will then be used to grab further information about the charter not located on the ASC page. We will also grab the link to people appearing on the charter from each ASC page which we will use in the following step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Define Object to Scrape Info from a ASC Charter Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASC Charter Scraper Defined\n"
     ]
    }
   ],
   "source": [
    "class ASCCharterPage:\n",
    "    \"\"\"Extracts data from a single charter from the ASC database.\"\"\"\n",
    "    \n",
    "    def __init__(self, url, options={'delay': 0.1}):\n",
    "        self._url = url\n",
    "        # fetch page and parse into beautifulsoup object\n",
    "        with dhelp.WebPage(self._url) as page_soup:\n",
    "            # get only portion of page with charter-specific content\n",
    "            self.data = page_soup.body.div.table.find('td', id='content').div\n",
    "        # eager load navbar to speed up link retreival\n",
    "        self._navbar = self.data.find('ul', class_='charter-nav')\n",
    "            \n",
    "    @property\n",
    "    def id(self):\n",
    "        \"\"\"Gives the charter ID, with spaces removed.\"\"\"\n",
    "        return self.data.div.div.h1.get_text().replace(' ', '')\n",
    "        \n",
    "    @property\n",
    "    def pase_source(self):\n",
    "        \"\"\"Url to source in PASE database, if extant, otherwise None.\"\"\"\n",
    "        raw_url = self._navbar.find('li', class_='charter-pase-source').find('a')['href']\n",
    "        # fixes link on asc page, which points to an obsolete address\n",
    "        return raw_url.replace('ASC', 'Sources').replace('source.jsp', 'DisplaySource.jsp')\n",
    "        \n",
    "    @property\n",
    "    def pase_witnesses(self):\n",
    "        \"\"\"URL to list of people appearing on charter in the PASE database.\"\"\"\n",
    "        try:\n",
    "            return self._navbar.find('li', class_='charter-pase-witnesses').find('a')['href']\n",
    "        # return None if no witnesses are found (i.e. no link exists)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def description(self):\n",
    "        \"\"\"Modern description of charter.\"\"\"\n",
    "        return self.data.p.get_text()\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        \"\"\"Full text of the charter, in original language. Editorial clause markings removed.\"\"\"\n",
    "        # grab text and convert from latin-1 to utf-8 encoding\n",
    "        raw_text = self.data.find_all('div')[3].get_text()\n",
    "        clean_text = bytearray(raw_text, 'latin-1').decode('utf-8')\n",
    "        # remove text of embedded editorial marks\n",
    "        remove_phrases = [\n",
    "            'DATING CLAUSE', 'INVOCATION', 'PROMULGATION PLACE', 'CURSE',\n",
    "            'DISPOSITIVE WORD', 'BOUNDS', 'PROEM',\n",
    "        ]\n",
    "        for remove_phrase in remove_phrases:\n",
    "            clean_text = clean_text.replace(remove_phrase, '')\n",
    "        # removes extra whitespace by spliting into list of words and rejoining\n",
    "        return dhelp.LatinText(clean_text).rm_spaces().stringify()\n",
    "    \n",
    "print('ASC Charter Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Define Object to Scrape Info from a PASE Charter Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASE Charter Scraper Defined\n"
     ]
    }
   ],
   "source": [
    "class PASECharterPage:\n",
    "    \"\"\"Extracts further information about charters from their PASE page.\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        # fetch page and convert to beautifulsoup object\n",
    "        with dhelp.WebPage(url, options={'delay': 0.1}) as page_soup:\n",
    "            self.data = page_soup\n",
    "        sections = self.data.find_all('div', class_='t01')\n",
    "        self._charter_info = sections[0].table.tr.td.table.find_all('tr')\n",
    "        self._source_info = sections[1].table.tr.td.table.find_all('tr')\n",
    "    \n",
    "    @property\n",
    "    def sawyer(self):\n",
    "        \"\"\"Returns the sawyer number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def birch(self):\n",
    "        \"\"\"Returns the birch number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def kemble(self):\n",
    "        \"\"\"Returns the kemble number if extant, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[2].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def british_academy(self):\n",
    "        \"\"\"Returns the British Academy reference, otherwise None.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[3].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def source_used(self):\n",
    "        \"\"\"Gives modern source used.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[4].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def archive(self):\n",
    "        \"\"\"Gives name of modern archive housing the charter.\"\"\"\n",
    "        try:\n",
    "            return self._charter_info[5].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def language(self):\n",
    "        \"\"\"Gives language(s) used in charter.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[0].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def date(self):\n",
    "        \"\"\"Gives long-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[1].td.get_text()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def scholarly_date(self):\n",
    "        \"\"\"Gives short-form version of date.\"\"\"\n",
    "        try:\n",
    "            return self._source_info[2].td.get_text()\n",
    "            \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_low(self):\n",
    "        \"\"\"Will return low date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[0].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_high(self):\n",
    "        \"\"\"Will return high date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        try:\n",
    "            return int(self.scholarly_date.split('x')[1].replace(' ', ''))\n",
    "        except:\n",
    "            return int(self.scholarly_date)\n",
    "        \n",
    "    @property\n",
    "    def scholarly_date_avg(self):\n",
    "        \"\"\"Returns mean date if date range exists, otherwise return scholarly_date\"\"\"\n",
    "        low_date = self.scholarly_date_low\n",
    "        high_date = self.scholarly_date_high\n",
    "        if low_date == high_date:\n",
    "            return low_date\n",
    "        else:\n",
    "            return round((low_date + high_date) / 2, 2)\n",
    "    \n",
    "    @property\n",
    "    def notes(self):\n",
    "        \"\"\"Gives miscellaneous notes on charter.\"\"\"\n",
    "        try:\n",
    "            return self.data.find('div', class_='rec').p.get_text()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "print('PASE Charter Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Scrape Charter Info from ASC and PASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering charters (467)....\n",
      "Fetching http://www.aschart.kcl.ac.uk/charters/s1482.html\n",
      "Successfully scraped http://www.aschart.kcl.ac.uk/charters/s1482.html\n",
      "Fetching http://www.pase.ac.uk/jsp/Sources/DisplaySource.jsp?sourceKey=1972\n",
      "Successfully scraped http://www.pase.ac.uk/jsp/Sources/DisplaySource.jsp?sourceKey=1972\n",
      "Error loading page at http://www.aschart.kcl.ac.uk/charters/s1482.html (skipped)\n",
      "Charters successfully scraped\n"
     ]
    }
   ],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# before scraping information, we need to get the urls for every ASC charter\n",
    "asc_links = []\n",
    "charter_soup = None\n",
    "# fetch page and parse into beautifulsoup object\n",
    "with dhelp.WebPage('http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html') as page_soup:\n",
    "    # get only portion of page with charter-specific content\n",
    "    charter_soup = page_soup.body.div.table.find('tr', class_='r02').find('td', id='content').div\n",
    "# looping through each section and group of rulers\n",
    "for ruler_section in charter_soup.find_all('ul', class_='asc-expand'):\n",
    "    for ruler_group in ruler_section.find_all('li'):\n",
    "        # get relative links from <a> tags append full link to self.data by adding root_url\n",
    "        for charter_link_wrapper in ruler_group.find_all('li'):\n",
    "            asc_links.append('http://www.aschart.kcl.ac.uk' + charter_link_wrapper.a['href'])\n",
    "\n",
    "charter_counter = 0\n",
    "# loop through each charter found online\n",
    "for charter_url in asc_links:\n",
    "    clear_output()\n",
    "    print('Gathering charters ({})....'.format(charter_counter + 1))\n",
    "    try:\n",
    "        asc_charter_page = ASCCharterPage(charter_url)\n",
    "        pase_charter_page = PASECharterPage(asc_charter_page.pase_source)\n",
    "        # make new charter and add to session\n",
    "        session.add(Charter(\n",
    "            id=asc_charter_page.id,\n",
    "            description=asc_charter_page.description,\n",
    "            sawyer=pase_charter_page.sawyer,\n",
    "            birch=pase_charter_page.birch,\n",
    "            kemble=pase_charter_page.kemble,\n",
    "            british_academy=pase_charter_page.british_academy,\n",
    "            source_used=pase_charter_page.source_used,\n",
    "            archive=pase_charter_page.archive,\n",
    "            language=pase_charter_page.language,\n",
    "            date=pase_charter_page.date,\n",
    "            scholarly_date=pase_charter_page.scholarly_date,\n",
    "            scholarly_date_low=pase_charter_page.scholarly_date_low,\n",
    "            scholarly_date_high=pase_charter_page.scholarly_date_high,\n",
    "            scholarly_date_avg=pase_charter_page.scholarly_date_avg,\n",
    "            text=asc_charter_page.text,\n",
    "            notes=pase_charter_page.notes,\n",
    "            asc_source=asc_charter_page._url,\n",
    "            pase_source=asc_charter_page.pase_source,\n",
    "            pase_witnesses=asc_charter_page.pase_witnesses\n",
    "        ))\n",
    "    except:\n",
    "        print('Error loading page at', charter_url, '(skipped)')\n",
    "    charter_counter += 1\n",
    "# commit all changes to the local db\n",
    "session.commit()\n",
    "session.close()\n",
    "\n",
    "print('Charters successfully scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Scrape PASE for People in Charters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) Define Object to Scrape Info from a PASE Charter Witnesses Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASE Witnesses Scraper Defined\n"
     ]
    }
   ],
   "source": [
    "class PASEWitnesses:\n",
    "    \"\"\"Gets basic information about people on the charter from a PASE witnesses page\"\"\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self._url = url\n",
    "        with dhelp.WebPage(self._url, options={'delay': 0.1}) as page_soup:\n",
    "            self._soup = page_soup\n",
    "            # get only portion of page with relevant data\n",
    "            try:\n",
    "                self.data = page_soup.find('div', class_='rec').find('ul').find_all('li')\n",
    "            # sometimes notes preceed data, in which case get second div with class rec\n",
    "            except:\n",
    "                self.data = page_soup.find_all('div', class_='rec')[1].find('ul').find_all('li')\n",
    "    @property\n",
    "    def witnesses(self):\n",
    "        witness_list = []\n",
    "        for witness_entry in self.data:\n",
    "            witness_link_element = witness_entry.find('a')\n",
    "            try:\n",
    "                witness_role = witness_entry.find('strong').get_text()\n",
    "            except:\n",
    "                witness_role = 'Witness'\n",
    "            witness_list.append({\n",
    "                    'role': witness_role,\n",
    "                    'name': witness_link_element.get_text(),\n",
    "                    'link': witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                    'description': witness_entry.find('em').get_text()\n",
    "                })\n",
    "            # look for nested witnesses, sometimes buried in recursive em tags\n",
    "            nested_witnesses_element = None\n",
    "            try:\n",
    "                nested_witnesses_element = self._soup.find('div', class_='rec').find('ul').find('em')\n",
    "            except:\n",
    "                nested_witnesses_element = self._soup.find_all('div', class_='rec')[1].find('ul').find('em')\n",
    "            while nested_witnesses_element is not None:\n",
    "                nested_witnesses = nested_witnesses_element.find_all('li')\n",
    "                for nested_witness_entry in nested_witnesses:\n",
    "                    nested_witness_link_element = witness_entry.find('a')\n",
    "                    try:\n",
    "                        nested_witness_role = nested_witness_entry.find('strong').get_text()\n",
    "                    except:\n",
    "                        nested_witness_role = 'Witness'\n",
    "                    witness_list.append({\n",
    "                        'role': nested_witness_role,\n",
    "                        'name': nested_witness_link_element.get_text(),\n",
    "                        'link': nested_witness_link_element['href'].replace('../', 'http://www.pase.ac.uk/jsp/'),\n",
    "                        'description': nested_witness_entry.find('em').get_text()\n",
    "                    })\n",
    "                nested_witnesses_element = nested_witnesses_element.find('em')\n",
    "        return witness_list\n",
    "    \n",
    "\n",
    "print('PASE Witnesses Scraper Defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) Scrape Witness Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering charter witnesses from charter (347)....\n",
      "Fetching http://www.pase.ac.uk/jsp/ASC/factoid.jsp?factoidKey=51596\n",
      "Successfully scraped http://www.pase.ac.uk/jsp/ASC/factoid.jsp?factoidKey=51596\n",
      "Added person/charter relationship  Cenwulf 3 -> S1442\n",
      "Added person  Wullaf 5\n",
      "Added person/charter relationship  Wullaf 5 -> S1442\n",
      "Added person  Cynethryth 4\n",
      "Added person/charter relationship  Cynethryth 4 -> S1442\n",
      "Added person  Anonymous 689\n",
      "Added person/charter relationship  Anonymous 689 -> S1442\n",
      "Added person  Ælfflæd 8\n",
      "Added person/charter relationship  Ælfflæd 8 -> S1442\n",
      "Added person/charter relationship  Æthelred 1 -> S1442\n",
      "Added person  Worcester 1\n",
      "Added person/charter relationship  Worcester 1 -> S1442\n",
      "Added person  Winchcombe 1\n",
      "Added person/charter relationship  Winchcombe 1 -> S1442\n",
      "Added person/charter relationship  Wærfrith 6 -> S1442\n",
      "Added person/charter relationship  Beorhthun 6 -> S1442\n",
      "Added person/charter relationship  Beorhtmund 4 -> S1442\n",
      "Added person/charter relationship  Tidbald 5 -> S1442\n",
      "Added person  Cynehelm 8\n",
      "Added person/charter relationship  Cynehelm 8 -> S1442\n",
      "Added person/charter relationship  Ecgfrith 12 -> S1442\n",
      "Added person/charter relationship  Wigheard 11 -> S1442\n",
      "Added person/charter relationship  Æthelwulf 21 -> S1442\n",
      "Added person  Wigred 3\n",
      "Added person/charter relationship  Wigred 3 -> S1442\n",
      "Added person  Cenberht 4\n",
      "Added person/charter relationship  Cenberht 4 -> S1442\n",
      "Added person/charter relationship  Wynnhelm 4 -> S1442\n",
      "Added person  Ealhhelm 6\n",
      "Added person/charter relationship  Ealhhelm 6 -> S1442\n",
      "Added person  Deorberht 1\n",
      "Added person/charter relationship  Deorberht 1 -> S1442\n",
      "Added person  Æthelswith 3\n",
      "Added person/charter relationship  Æthelswith 3 -> S1442\n",
      "Added person  Wigswith 1\n",
      "Added person/charter relationship  Wigswith 1 -> S1442\n",
      "Added person  Lulla 10\n",
      "Added person/charter relationship  Lulla 10 -> S1442\n",
      "Witnesses successfully scraped\n"
     ]
    }
   ],
   "source": [
    "# first, we need to open a session with the local database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session(autoflush=False)\n",
    "\n",
    "charter_counter = 0\n",
    "# now we want to query our local db for every charter, which is returned as a Charter object\n",
    "for charter in session.query(Charter):\n",
    "    clear_output()\n",
    "    print('Gathering charter witnesses from charter ({})....'.format(charter_counter + 1))\n",
    "    # then get the link to the corresponding PASE page with witness information, skip if there is no link\n",
    "    witnesses_link = charter.pase_witnesses\n",
    "    if witnesses_link is not None:\n",
    "        # use the scraper to get witness data as list of dicts, only proceed if results were found\n",
    "        witness_list = PASEWitnesses(witnesses_link).witnesses\n",
    "        # loop through the list of witnesses\n",
    "        for witness in witness_list:\n",
    "            # query to see if person already exists in db, if no results found, then add them\n",
    "            person_query = session.query(Person).filter(Person.id == witness['name'])\n",
    "            # if any results are in the list, it will set person_found to True\n",
    "            person_found = False\n",
    "            for person in person_query:\n",
    "                person_found = True\n",
    "            if not person_found:\n",
    "                try:\n",
    "                    session.add(Person(\n",
    "                        id=witness['name'],\n",
    "                        description=witness['description'],\n",
    "                        link=witness['link']\n",
    "                    ))\n",
    "                    print('Added person {}'.format(witness['name']))\n",
    "                    session.commit()\n",
    "                except:\n",
    "                    session.rollback()\n",
    "            # add charter/person relationship information to `charter_witnesses` table created above\n",
    "            try:\n",
    "                session.add(CharterWitness(\n",
    "                    charter_id=charter.id,\n",
    "                    person_id=witness['name'],\n",
    "                    role=witness['role'],\n",
    "                    link=str(witnesses_link)\n",
    "                ))\n",
    "                session.commit()\n",
    "            except:\n",
    "                session.rollback()\n",
    "            print('Added person/charter relationship {} -> {}'.format(witness['name'], charter.id))\n",
    "    charter_counter += 1\n",
    "    \n",
    "# commit all changes to the local db\n",
    "session.close()\n",
    "\n",
    "print('Witnesses successfully scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table data successfully exported\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "con = sqlite3.connect('charters.db')\n",
    "\n",
    "\n",
    "def dump_table(tablename, filename):\n",
    "    with open(filename, 'w+') as outfile:\n",
    "        outcsv = csv.writer(outfile)\n",
    "        cursor = con.execute('select * from ' + tablename)\n",
    "        # dump column titles (optional)\n",
    "        outcsv.writerow(x[0] for x in cursor.description)\n",
    "        # dump rows\n",
    "        outcsv.writerows(cursor.fetchall())\n",
    "    return True\n",
    "\n",
    "if (\n",
    "    dump_table('charters', 'export_charters.csv') and\n",
    "    dump_table('people', 'export_people.csv') and\n",
    "    dump_table('charter_witnesses', 'export_charter_witnesses.csv')\n",
    "):\n",
    "    print('Table data successfully exported')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
